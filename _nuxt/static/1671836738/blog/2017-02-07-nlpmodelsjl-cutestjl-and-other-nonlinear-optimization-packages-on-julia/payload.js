__NUXT_JSONP__("/blog/2017-02-07-nlpmodelsjl-cutestjl-and-other-nonlinear-optimization-packages-on-julia", (function(a,b,c,d,e,f,g,h,i,j,k,l,m,n,o,p,q,r,s,t,u,v,w,x,y,z,A,B,C,D,E){return {data:[{page:{slug:"2017-02-07-nlpmodelsjl-cutestjl-and-other-nonlinear-optimization-packages-on-julia",title:z,date:"2017-02-07",tags:["julia","optimization","nlpmodels","cutest","work","juliasmoothoptimizers","tutorial"],toc:[],body:{type:"root",children:[{type:b,tag:"h1",props:{id:"nlpmodelsjl-cutestjl-and-other-nonlinear-optimization-packages-on-julia"},children:[{type:b,tag:k,props:{href:"#nlpmodelsjl-cutestjl-and-other-nonlinear-optimization-packages-on-julia",ariaHidden:"true",tabIndex:-1},children:[{type:b,tag:"span",props:{className:["icon","icon-link"]},children:[]}]},{type:a,value:z}]},{type:a,value:c},{type:b,tag:d,props:{},children:[{type:a,value:"A couple of weeks ago me and Professor "},{type:b,tag:k,props:{href:"https:\u002F\u002Fdpo.github.io",rel:[l,m,n],target:o},children:[{type:a,value:"Dominique Orban"}]},{type:a,value:" have finally made a release of\nCUTEst.jl, a wrapper for the CUTEst repository of problems for nonlinear\noptimization (which I've mentioned before).\nAlong with this release, we've done a release of NLPModels.jl, the underlying\npackage. I think it's time I explain a little about these packages, others,\nand how to use them together.\nIf you want to see the output of the commands, you can open\n"},{type:b,tag:k,props:{href:"https:\u002F\u002Fasciinema.org\u002Fa\u002F102371",rel:[l,m,n],target:o},children:[{type:a,value:"this ASCIInema"}]},{type:a,value:"\nside by side."}]},{type:a,value:c},{type:b,tag:d,props:{},children:[{type:b,tag:r,props:{},children:[{type:a,value:"Obs.: Tutorial using Julia 0.5.0"}]}]},{type:a,value:c},{type:b,tag:d,props:{},children:[{type:b,tag:r,props:{},children:[{type:a,value:"Edit: Second part is\n"},{type:b,tag:k,props:{href:"https:\u002F\u002Fabelsiqueira.github.io%7B%7Blocal_prefix%7D%7Dnlpmodelsjl-and-cutestjl-constrained-optimization\u002F",rel:[l,m,n],target:o},children:[{type:a,value:A}]},{type:a,value:t}]}]},{type:a,value:c},{type:b,tag:d,props:{},children:[{type:b,tag:p,props:{},children:[{type:a,value:"JuliaSmoothOptimizers"}]},{type:a,value:c},{type:b,tag:k,props:{href:"https:\u002F\u002Fjuliasmoothoptimizers.github.io",rel:[l,m,n],target:o},children:[{type:b,tag:"img",props:{alt:"JuliaSmoothOptimizers logo",src:"https:\u002F\u002Fjuliasmoothoptimizers.github.io\u002Fassets\u002Flogo.png"},children:[]},{type:a,value:"{: .img-view }"}]}]},{type:a,value:c},{type:b,tag:d,props:{},children:[{type:a,value:"Most packages mentioned here will be a part of the JuliaSmoothOptimizers (JSO)\norganization. There are more packages in the organization that I won't mention here, but you should check it out."}]},{type:a,value:c},{type:b,tag:d,props:{},children:[{type:b,tag:p,props:{},children:[{type:a,value:w}]}]},{type:a,value:c},{type:b,tag:d,props:{},children:[{type:a,value:"NLPModels is a package for creating Nonlinear Optimization Models. It is\nfocused on the needs of the solver writer, such as the ability to write one\nsolver that works on many models.\nThe package defines a few models, and there are others on the horizon.\nThe ones already done are:"}]},{type:a,value:c},{type:b,tag:B,props:{},children:[{type:a,value:c},{type:b,tag:q,props:{},children:[{type:b,tag:p,props:{},children:[{type:a,value:x}]},{type:a,value:": A model with automatic differentiation;"}]},{type:a,value:c},{type:b,tag:q,props:{},children:[{type:b,tag:p,props:{},children:[{type:a,value:y}]},{type:a,value:": A model for "},{type:b,tag:k,props:{href:"https:\u002F\u002Fgithub.com\u002FJuliaOpt\u002FMathProgBase.jl",rel:[l,m,n],target:o},children:[{type:a,value:"MathProgBase"}]},{type:a,value:"\u002F"},{type:b,tag:k,props:{href:"http:\u002F\u002Fgithub.com\u002FJuliaOpt\u002FJuMP.jl",rel:[l,m,n],target:o},children:[{type:a,value:C}]},{type:a,value:" conversion, whose utility will be shown below (obs: MPB and JuMP are packages from the JuliaOpt organization);"}]},{type:a,value:c},{type:b,tag:q,props:{},children:[{type:b,tag:p,props:{},children:[{type:a,value:"SimpleNLPModel"}]},{type:a,value:": A model in which nothing is automatic, i.e., all functions have to be provided by the user."}]},{type:a,value:c},{type:b,tag:q,props:{},children:[{type:b,tag:p,props:{},children:[{type:a,value:"SlackModel"}]},{type:a,value:": A model that changes all inequalities to equalities adding extra variables;"}]},{type:a,value:c},{type:b,tag:q,props:{},children:[{type:b,tag:p,props:{},children:[{type:a,value:"LBFGSModel"}]},{type:a,value:u},{type:b,tag:p,props:{},children:[{type:a,value:"LSR1Model"}]},{type:a,value:": Models that create quasi-Newton models from another model."}]},{type:a,value:c}]},{type:a,value:c},{type:b,tag:d,props:{},children:[{type:a,value:"The first two models are designed to be easy to use; the third is useful for\nefficient model creation in specific cases; the last ones are utility models."}]},{type:a,value:c},{type:b,tag:d,props:{},children:[{type:a,value:"Let's begin by installing NLPModels.jl, and a couple of optional requirements."}]},{type:a,value:c},{type:b,tag:f,props:{className:[g]},children:[{type:b,tag:h,props:{className:[i,j]},children:[{type:b,tag:e,props:{},children:[{type:a,value:"Pkg.add(\"NLPModels.jl\")\nPkg.add(\"JuMP.jl\") # Installs ForwardDiff also.\n"}]}]}]},{type:a,value:c},{type:b,tag:d,props:{},children:[{type:a,value:"This should install version 0.1.0. After that, just do"}]},{type:a,value:c},{type:b,tag:f,props:{className:[g]},children:[{type:b,tag:h,props:{className:[i,j]},children:[{type:b,tag:e,props:{},children:[{type:a,value:"using NLPModels\n"}]}]}]},{type:a,value:c},{type:b,tag:d,props:{},children:[{type:a,value:"Now, let's create a simple function: Rosenbrock's."}]},{type:a,value:c},{type:b,tag:f,props:{className:[g]},children:[{type:b,tag:h,props:{className:[i,j]},children:[{type:b,tag:e,props:{},children:[{type:a,value:"f(x) = (x[1] - 1)^2 + 100*(x[2] - x[1]^2)^2\n"}]}]}]},{type:a,value:c},{type:b,tag:d,props:{},children:[{type:a,value:"The Rosenbrock problem traditionally starts from $(-1.2,1.0)$."}]},{type:a,value:c},{type:b,tag:f,props:{className:[g]},children:[{type:b,tag:h,props:{className:[i,j]},children:[{type:b,tag:e,props:{},children:[{type:a,value:"x0 = [-1.2; 1.0]\n"}]}]}]},{type:a,value:c},{type:b,tag:d,props:{},children:[{type:a,value:"Now, we are ready to create the problem."}]},{type:a,value:c},{type:b,tag:f,props:{className:[g]},children:[{type:b,tag:h,props:{className:[i,j]},children:[{type:b,tag:e,props:{},children:[{type:a,value:"adnlp = ADNLPModel(f, x0)\n"}]}]}]},{type:a,value:c},{type:b,tag:d,props:{},children:[{type:a,value:"Now, we can access the function and derivatives using the "},{type:b,tag:k,props:{href:"https:\u002F\u002Fjuliasmoothoptimizers.github.io\u002FNLPModels.jl\u002Fstable\u002Fapi.html",rel:[l,m,n],target:o},children:[{type:a,value:"NLPModels API"}]}]},{type:a,value:c},{type:b,tag:f,props:{className:[g]},children:[{type:b,tag:h,props:{className:[i,j]},children:[{type:b,tag:e,props:{},children:[{type:a,value:"obj(adnlp, adnlp.meta.x0)\ngrad(adnlp, adnlp.meta.x0)\nhess(adnlp, adnlp.meta.x0)\nobjgrad(adnlp, adnlp.meta.x0)\nhprod(adnlp, adnlp.meta.x0, ones(2))\nH = hess_op(adnlp, adnlp.meta.x0)\nH * ones(2)\n"}]}]}]},{type:a,value:c},{type:b,tag:d,props:{},children:[{type:a,value:"At this point, we can't differentiate many things from simply using\n"},{type:b,tag:e,props:{},children:[{type:a,value:"ForwardDiff"}]},{type:a,value:" interface directly, but two things stand out: "},{type:b,tag:e,props:{},children:[{type:a,value:"objgrad"}]},{type:a,value:" returns\nboth functions at once, and "},{type:b,tag:e,props:{},children:[{type:a,value:s}]},{type:a,value:" returns a\n"},{type:b,tag:k,props:{href:"https:\u002F\u002Fgithub.com\u002FJuliaSmoothOptimizers\u002FLinearOperators.jl",rel:[l,m,n],target:o},children:[{type:a,value:"LinearOperator"}]},{type:a,value:",\nanother structure created in JuliaSmoothOptimizers.\nThis one defines a linear operator, extending Julia matrices in the sense that if"}]},{type:a,value:c},{type:b,tag:f,props:{className:[g]},children:[{type:b,tag:h,props:{className:[i,j]},children:[{type:b,tag:e,props:{},children:[{type:a,value:"using LinearOperators\nn = 100\nA = rand(n, n)\nB = rand(n, n)\nopA = LinearOperator(A)\nopB = LinearOperator(B)\nv = rand(n)\n"}]}]}]},{type:a,value:c},{type:b,tag:d,props:{},children:[{type:a,value:"then "},{type:b,tag:e,props:{},children:[{type:a,value:"(A * B) * v"}]},{type:a,value:" computes the matrix product, whereas "},{type:b,tag:e,props:{},children:[{type:a,value:"(opA * opB) * v"}]},{type:a,value:" won't.\nFurthermore, the linear operator can be created from the functions\n"},{type:b,tag:e,props:{},children:[{type:a,value:"v-\u003EMp(v)"}]},{type:a,value:u},{type:b,tag:e,props:{},children:[{type:a,value:"v-\u003EMtp(v)"}]},{type:a,value:", defining the product of the linear operator times a vector and its transpose times a vector."}]},{type:a,value:c},{type:b,tag:f,props:{className:[g]},children:[{type:b,tag:h,props:{className:[i,j]},children:[{type:b,tag:e,props:{},children:[{type:a,value:"T = LinearOperator(2, 2, # sizes\n                   false, false,\n                   v-\u003E[-v[2]; v[1]], v-\u003E[v[2]; -v[1]])\nv = rand(2)\nT * v\nT' * v\n"}]}]}]},{type:a,value:c},{type:b,tag:d,props:{},children:[{type:b,tag:r,props:{},children:[{type:a,value:"Obs: In the "},{type:b,tag:e,props:{},children:[{type:a,value:x}]},{type:a,value:" case, "},{type:b,tag:e,props:{},children:[{type:a,value:s}]},{type:a,value:" returns a linear operator that is actually\ncomputing the matrix, but this is a issue to be tackled on the future (PRs\nwelcome). But we'll be back with uses for "},{type:b,tag:e,props:{},children:[{type:a,value:s}]},{type:a,value:" soon."}]}]},{type:a,value:c},{type:b,tag:d,props:{},children:[{type:a,value:"The next model is the "},{type:b,tag:e,props:{},children:[{type:a,value:y}]},{type:a,value:". This model's main use is the "},{type:b,tag:e,props:{},children:[{type:a,value:C}]},{type:a,value:"\nmodelling language. This is very useful for more elaborate writing, specially\nwith constraints. It does create a little more overhead though, so keep that\nin mind."}]},{type:a,value:c},{type:b,tag:f,props:{className:[g]},children:[{type:b,tag:h,props:{className:[i,j]},children:[{type:b,tag:e,props:{},children:[{type:a,value:"using JuMP\njmp = Model()\n@variable(jmp, x[i=1:2], start=(x0[i])) # x0 from before\n@NLobjective(jmp, Min, (x[1] - 1)^2 + 100*(x[2] - x[1]^2)^2)\nmpbnlp = MathProgNLPModel(jmp)\n"}]}]}]},{type:a,value:c},{type:b,tag:d,props:{},children:[{type:a,value:"Try the commands again."}]},{type:a,value:c},{type:b,tag:f,props:{className:[g]},children:[{type:b,tag:h,props:{className:[i,j]},children:[{type:b,tag:e,props:{},children:[{type:a,value:"obj(mpbnlp, mpbnlp.meta.x0)\ngrad(mpbnlp, mpbnlp.meta.x0)\nhess(mpbnlp, mpbnlp.meta.x0)\nobjgrad(mpbnlp, mpbnlp.meta.x0)\nhprod(mpbnlp, mpbnlp.meta.x0, ones(2))\nH = hess_op(mpbnlp, mpbnlp.meta.x0)\nH * ones(2)\n"}]}]}]},{type:a,value:c},{type:b,tag:d,props:{},children:[{type:a,value:"It should be pretty much the same, though there is a little difference in "},{type:b,tag:e,props:{},children:[{type:a,value:"hess"}]},{type:a,value:".\nJuMP creates the sparse Hessian, which is better, from a computational point of\nview."}]},{type:a,value:c},{type:b,tag:d,props:{},children:[{type:a,value:"Notice how the commands are the same. I've actually copy-pasted the commands\nfrom above.\nThis allows the write of a solver in just a couple of commands.\nFor instance, a simple "},{type:b,tag:p,props:{},children:[{type:a,value:"Newton method"}]},{type:a,value:t}]},{type:a,value:c},{type:b,tag:f,props:{className:[g]},children:[{type:b,tag:h,props:{className:[i,j]},children:[{type:b,tag:e,props:{},children:[{type:a,value:"function newton(nlp :: AbstractNLPModel)\n  x = nlp.meta.x0\n  fx = obj(nlp, x)\n  gx = grad(nlp, x)\n  ngx = norm(gx)\n  while ngx \u003E 1e-6\n    Hx = hess(nlp, x)\n    d = -gx\n    try\n      G = chol(Hermitian(Hx, :L)) # Make Cholesky work on lower-only matrix.\n      d = -G\\(G'\\gx)\n    catch e\n      if !isa(e, Base.LinAlg.PosDefException); rethrow(e); end\n    end\n    t = 1.0\n    xt = x + t * d\n    ft = obj(nlp, xt)\n    while ft \u003E fx + 0.5 * t * dot(gx, d)\n      t *= 0.5\n      xt = x + t * d\n      ft = obj(nlp, xt)\n    end\n    x = xt\n    fx = ft\n    gx = grad(nlp, x)\n    ngx = norm(gx)\n  end\n  return x, fx, ngx\nend\n"}]}]}]},{type:a,value:c},{type:b,tag:d,props:{},children:[{type:a,value:"And we run in the problems with"}]},{type:a,value:c},{type:b,tag:f,props:{className:[g]},children:[{type:b,tag:h,props:{className:[i,j]},children:[{type:b,tag:e,props:{},children:[{type:a,value:"newton(adnlp)\nnewton(mpbnlp)\n"}]}]}]},{type:a,value:c},{type:b,tag:d,props:{},children:[{type:b,tag:r,props:{},children:[{type:a,value:"Write once, use on problems from different sources."}]}]},{type:a,value:c},{type:b,tag:d,props:{},children:[{type:a,value:"Now, to have more fun, let's get another package:\n"},{type:b,tag:k,props:{href:"https:\u002F\u002Fgithub.com\u002FJuliaSmoothOptimizers\u002FOptimizationProblems.jl",rel:[l,m,n],target:o},children:[{type:a,value:D}]},{type:a,value:".\nThis package doesn't have a release yet, so we have to clone it:"}]},{type:a,value:c},{type:b,tag:f,props:{className:[g]},children:[{type:b,tag:h,props:{className:[i,j]},children:[{type:b,tag:e,props:{},children:[{type:a,value:"Pkg.clone(\"https:\u002F\u002Fgithub.com\u002FJuliaSmoothOptimizers\u002FOptimizationProblems.jl\")\n"}]}]}]},{type:a,value:c},{type:b,tag:d,props:{},children:[{type:a,value:"What we have here is a collection of JuMP models implementing some of the\nCUTEst problems. Together with "},{type:b,tag:e,props:{},children:[{type:a,value:w}]},{type:a,value:", we have a good opportunity to test our Newton implementation."}]},{type:a,value:c},{type:b,tag:f,props:{className:[g]},children:[{type:b,tag:h,props:{className:[i,j]},children:[{type:b,tag:e,props:{},children:[{type:a,value:"using OptimizationProblems\n\nx, fx, ngx = newton(MathProgNLPModel( rosenbrock() ))\nx, fx, ngx = newton(MathProgNLPModel( dixmaanj() ))\nx, fx, ngx = newton(MathProgNLPModel( brownbs() ))\n"}]}]}]},{type:a,value:c},{type:b,tag:d,props:{},children:[{type:b,tag:r,props:{},children:[{type:a,value:"An issue with OptimizationProblems is that it still doesn't have a way to get\nall unconstrained problems, for instance. (PRs are welcome)."}]}]},{type:a,value:c},{type:b,tag:d,props:{},children:[{type:a,value:"So far we used 3 packages from JSO: "},{type:b,tag:e,props:{},children:[{type:a,value:w}]},{type:a,value:", "},{type:b,tag:e,props:{},children:[{type:a,value:"LinearOperators.jl"}]},{type:a,value:u},{type:b,tag:e,props:{},children:[{type:a,value:D}]},{type:a,value:". It's time to meet another important package."}]},{type:a,value:c},{type:b,tag:d,props:{},children:[{type:b,tag:p,props:{},children:[{type:a,value:"CUTEst.jl"}]}]},{type:a,value:c},{type:b,tag:d,props:{},children:[{type:a,value:"CUTEst, the Constrained and Unconstrained Testing Environment with safe\nthreads, is a package written in Fortran providing over a thousand problems to\nallow testing of Nonlinear Programming solvers. However, CUTEst is hard to use\nby first-timers. Just installing it was already hard.\nCUTEst.jl provides an interface for CUTEst that is simple to install and use\n(comparing to the original)."}]},{type:a,value:c},{type:b,tag:d,props:{},children:[{type:b,tag:r,props:{},children:[{type:a,value:"Obs.: CUTEst.jl does not work on Windows for now. In fact, there is no plan to\nmake it work on Windows because \"people interested in doing it\"∩\"people capable\nof doing it\" = ∅, as far as we've looked. If you are in this set, PRs are\nwelcome."}]}]},{type:a,value:c},{type:b,tag:d,props:{},children:[{type:a,value:"To install CUTEst.jl you need to install something manually. Unfortunately,\nthis is specific for each system. Except for OSX, actually, which is using\n"},{type:b,tag:k,props:{href:"https:\u002F\u002Fgithub.com\u002Foptimizers\u002Fhomebrew-cutest",rel:[l,m,n],target:o},children:[{type:a,value:"homebrew-cutest"}]},{type:a,value:t}]},{type:a,value:c},{type:b,tag:d,props:{},children:[{type:a,value:"For Linux users, check out "},{type:b,tag:k,props:{href:"http:\u002F\u002Fjuliasmoothoptimizers.github.io\u002FCUTEst.jl\u002Flatest\u002F#Installing-1",rel:[l,m,n],target:o},children:[{type:a,value:"this\npage"}]},{type:a,value:".\nEssentially, we need "},{type:b,tag:e,props:{},children:[{type:a,value:"libgfortran.so"}]},{type:a,value:" in a visible place. And it's especially\nannoying that some distritions don't put it in a visible place."}]},{type:a,value:c},{type:b,tag:d,props:{},children:[{type:a,value:"With that done, enter"}]},{type:a,value:c},{type:b,tag:f,props:{className:[g]},children:[{type:b,tag:h,props:{className:[i,j]},children:[{type:b,tag:e,props:{},children:[{type:a,value:"Pkg.add(\"CUTEst\")\n"}]}]}]},{type:a,value:c},{type:b,tag:d,props:{},children:[{type:a,value:"which should install CUTEst.jl 0.1.0."}]},{type:a,value:c},{type:b,tag:d,props:{},children:[{type:a,value:"Yes, it takes some time."}]},{type:a,value:c},{type:b,tag:d,props:{},children:[{type:a,value:"Finally, we start using CUTEst with"}]},{type:a,value:c},{type:b,tag:f,props:{className:[g]},children:[{type:b,tag:h,props:{className:[i,j]},children:[{type:b,tag:e,props:{},children:[{type:a,value:"using CUTEst\n\nnlp = CUTEstModel(\"ROSENBR\")\n"}]}]}]},{type:a,value:c},{type:b,tag:d,props:{},children:[{type:b,tag:e,props:{},children:[{type:a,value:"ROSENBR"}]},{type:a,value:" is a CUTEst problem, in case you want the list, see\n"},{type:b,tag:k,props:{href:"http:\u002F\u002Fwww.cuter.rl.ac.uk\u002FProblems\u002Fmastsif.html",rel:[l,m,n],target:o},children:[{type:a,value:A}]},{type:a,value:". Keep reading for a way\nto select them, though."}]},{type:a,value:c},{type:b,tag:d,props:{},children:[{type:a,value:"Now, let's solve this CUTEst problem with our Newton method."}]},{type:a,value:c},{type:b,tag:f,props:{className:[g]},children:[{type:b,tag:h,props:{className:[i,j]},children:[{type:b,tag:e,props:{},children:[{type:a,value:"x, fx, ngx = newton(nlp)\n"}]}]}]},{type:a,value:c},{type:b,tag:d,props:{},children:[{type:b,tag:p,props:{},children:[{type:a,value:"Yes, exactly like before!"}]},{type:a,value:t}]},{type:a,value:c},{type:b,tag:d,props:{},children:[{type:a,value:"CUTEst is a little more annoying in other aspect also. Like, you can't have two\nor more problems open at the same time, and you have to close this problem\nbefore opening a new one. (Again, PRs are welcome)."}]},{type:a,value:c},{type:b,tag:f,props:{className:[g]},children:[{type:b,tag:h,props:{className:[i,j]},children:[{type:b,tag:e,props:{},children:[{type:a,value:"finalize(nlp)\nnlp = CUTEstModel(\"HIMMELBB\")\nx, fx, ngx = newton(nlp)\nfinalize(nlp)\n"}]}]}]},{type:a,value:c},{type:b,tag:d,props:{},children:[{type:a,value:"This allows a simple workflow for writing optimization solvers."}]},{type:a,value:c},{type:b,tag:B,props:{},children:[{type:a,value:c},{type:b,tag:q,props:{},children:[{type:a,value:"Write some problems by hand (using "},{type:b,tag:e,props:{},children:[{type:a,value:x}]},{type:a,value:" or "},{type:b,tag:e,props:{},children:[{type:a,value:y}]},{type:a,value:");"}]},{type:a,value:c},{type:b,tag:q,props:{},children:[{type:a,value:"Test your solvers with these hand-written problems;"}]},{type:a,value:c},{type:b,tag:q,props:{},children:[{type:a,value:"Repeat last two steps until you believe you're ready to competitive comparison;"}]},{type:a,value:c},{type:b,tag:q,props:{},children:[{type:a,value:"Test with CUTEst problems seamlessly."}]},{type:a,value:c}]},{type:a,value:c},{type:b,tag:d,props:{},children:[{type:a,value:"Now, let's get back to "},{type:b,tag:e,props:{},children:[{type:a,value:s}]},{type:a,value:". Remember that it used Matrix vector products?\nWell, CUTEst has separate functions for the product of the Hessian at a point\nand a vector. Which means that "},{type:b,tag:e,props:{},children:[{type:a,value:v}]},{type:a,value:" actually computes this product without\nhaving to create the matrix. Which means it is, at least, memory-efficient.\nFurthermore, "},{type:b,tag:e,props:{},children:[{type:a,value:s}]},{type:a,value:" will be created with the "},{type:b,tag:e,props:{},children:[{type:a,value:v}]},{type:a,value:" function, which means\nit is also memory-efficient."}]},{type:a,value:c},{type:b,tag:d,props:{},children:[{type:a,value:"Let's look at a huge problem to feel the difference."}]},{type:a,value:c},{type:b,tag:f,props:{className:[g]},children:[{type:b,tag:h,props:{className:[i,j]},children:[{type:b,tag:e,props:{},children:[{type:a,value:"nlp = CUTEstModel(\"BOX\")\nnlp.meta.nvar\n"}]}]}]},{type:a,value:c},{type:b,tag:d,props:{},children:[{type:a,value:"Let's make a simple comparison"}]},{type:a,value:c},{type:b,tag:f,props:{className:[g]},children:[{type:b,tag:h,props:{className:[i,j]},children:[{type:b,tag:e,props:{},children:[{type:a,value:"function foo1()\n  H = hess(nlp, nlp.meta.x0)\n  v = ones(nlp.meta.nvar)\n  return Hermitian(H, :L) * v\nend\n\nfunction foo2()\n  H = hess_op(nlp, nlp.meta.x0)\n  v = ones(nlp.meta.nvar)\n  return H * v\nend\n\n@time w1 = foo1();\n@time w2 = foo2();\nnorm(w1 - w2)\n"}]}]}]},{type:a,value:c},{type:b,tag:d,props:{},children:[{type:a,value:"Yes, that's a huge difference."}]},{type:a,value:c},{type:b,tag:d,props:{},children:[{type:a,value:"This is a very good reason to use "},{type:b,tag:e,props:{},children:[{type:a,value:s}]},{type:a,value:u},{type:b,tag:e,props:{},children:[{type:a,value:v}]},{type:a,value:". But let's take a step further."}]},{type:a,value:c},{type:b,tag:d,props:{},children:[{type:a,value:"We can't implement Cholesky using only "},{type:b,tag:e,props:{},children:[{type:a,value:v}]},{type:a,value:"s, so our Newton method would\nactually take a long time to reach a solution for the problem above.\nTo circunvent that, we could try using the Conjugate Gradients Method instead\nof Cholesky. This would only use Hessian-vector products."}]},{type:a,value:c},{type:b,tag:d,props:{},children:[{type:a,value:"We arrive on a new package,\n"},{type:b,tag:k,props:{href:"https:\u002F\u002Fgithub.com\u002FJuliaSmoothOptimizers\u002FKrylov.jl",rel:[l,m,n],target:o},children:[{type:a,value:"Krylov.jl"}]},{type:a,value:", which\nimplements Krylov methods. In particular, Conjugate Gradients.\nThis package is also unreleased, so we need to clone it."}]},{type:a,value:c},{type:b,tag:f,props:{className:[g]},children:[{type:b,tag:h,props:{className:[i,j]},children:[{type:b,tag:e,props:{},children:[{type:a,value:"Pkg.clone(\"https:\u002F\u002Fgithub.com\u002FJuliaSmoothOptimizers\u002FKrylov.jl\")\n"}]}]}]},{type:a,value:c},{type:b,tag:d,props:{},children:[{type:a,value:"Consider a simple example"}]},{type:a,value:c},{type:b,tag:f,props:{className:[g]},children:[{type:b,tag:h,props:{className:[i,j]},children:[{type:b,tag:e,props:{},children:[{type:a,value:"using Krylov\nA = rand(3,3)\nA = A*A'\nb = A*ones(3)\ncg(A, b)\n"}]}]}]},{type:a,value:c},{type:b,tag:d,props:{},children:[{type:a,value:"As expected, the system is solver, and the solution is $(1,1,1)$.\nBut let's do something more."}]},{type:a,value:c},{type:b,tag:f,props:{className:[g]},children:[{type:b,tag:h,props:{className:[i,j]},children:[{type:b,tag:e,props:{},children:[{type:a,value:"A = -A\ncg(A, b)\n"}]}]}]},{type:a,value:c},{type:b,tag:d,props:{},children:[{type:a,value:"Yes, Krylov does indeed solves the non-positive definite system using Conjugate Gradient.\nWell, actually, a variant."}]},{type:a,value:c},{type:b,tag:d,props:{},children:[{type:a,value:"That's not enough tough. Krylov.jl also accepts an additional argument "},{type:b,tag:e,props:{},children:[{type:a,value:"radius"}]},{type:a,value:"\nto set a trust-region radius."}]},{type:a,value:c},{type:b,tag:f,props:{className:[g]},children:[{type:b,tag:h,props:{className:[i,j]},children:[{type:b,tag:e,props:{},children:[{type:a,value:"cg(A, b, radius=0.1)\n"}]}]}]},{type:a,value:c},{type:b,tag:d,props:{},children:[{type:a,value:"Well, as an exercise I suggest you implement a trust-region version of Newton\nmethod, but for now, let's continue with our line-search version."}]},{type:a,value:c},{type:b,tag:d,props:{},children:[{type:a,value:"We know now how "},{type:b,tag:e,props:{},children:[{type:a,value:"cg"}]},{type:a,value:" behaves for non-positive definite systems, we can't make\nthe changes for a new method."}]},{type:a,value:c},{type:b,tag:f,props:{className:[g]},children:[{type:b,tag:h,props:{className:[i,j]},children:[{type:b,tag:e,props:{},children:[{type:a,value:"function newton2(nlp :: AbstractNLPModel)\n  x = nlp.meta.x0\n  fx = obj(nlp, x)\n  gx = grad(nlp, x)\n  ngx = norm(gx)\n  while norm(gx) \u003E 1e-6\n    Hx = hess_op(nlp, x)\n    d, _ = cg(Hx, -gx)\n    slope = dot(gx, d)\n    if slope \u003E= 0 # Not a descent direction\n      d = -gx\n      slope = -dot(d,d)\n    end\n    t = 1.0\n    xt = x + t * d\n    ft = obj(nlp, xt)\n    while ft \u003E fx + 0.5 * t * slope\n      t *= 0.5\n      xt = x + t * d\n      ft = obj(nlp, xt)\n    end\n    x = xt\n    fx = ft\n    gx = grad(nlp, x)\n    ngx = norm(gx)\n  end\n  return x, fx, ngx\nend\n"}]}]}]},{type:a,value:c},{type:b,tag:d,props:{},children:[{type:a,value:"Now, running "},{type:b,tag:e,props:{},children:[{type:a,value:"newton2"}]},{type:a,value:" on our large problem, we obtain"}]},{type:a,value:c},{type:b,tag:f,props:{className:[g]},children:[{type:b,tag:h,props:{className:[i,j]},children:[{type:b,tag:e,props:{},children:[{type:a,value:"x, fx, ngx = newton2(nlp)\n"}]}]}]},{type:a,value:c},{type:b,tag:d,props:{},children:[{type:a,value:"Which is the method working very fast. Less that a second here."}]},{type:a,value:c},{type:b,tag:"hr",props:{},children:[]},{type:a,value:c},{type:b,tag:d,props:{},children:[{type:a,value:"There is actually another package I'd like to talk about, but it needs some\nmore work for it to be ready for a release:"}]},{type:a,value:c},{type:b,tag:d,props:{},children:[{type:b,tag:p,props:{},children:[{type:a,value:"Optimize.jl"}]}]},{type:a,value:c},{type:b,tag:d,props:{},children:[{type:a,value:"Optimize.jl is a package with solvers. We intend to implement some high quality\nsolvers in there, but there is actually more to it. We have in there tools to\nbenchmark packages. These tools should allow the testing of a set of solvers in\na set of problems without much fuss, while creating the comparison information,\nincluding the performance profile.\nIt also includes, or will include, \"parts\" of solvers to create your own\nsolver. Like trust-region and line-search algorithms and auxiliary functions\nand types.\nUnfortunately, it's not done enough for me to extend on it, and this is already\ngetting too long."}]},{type:a,value:c},{type:b,tag:d,props:{},children:[{type:b,tag:p,props:{},children:[{type:a,value:"End"}]}]},{type:a,value:c},{type:b,tag:d,props:{},children:[{type:a,value:"I hope you enjoyed this overview of packages.\nSubscribe to the RSS feed to keep updated in future tutorials. I intend to talk\nabout the constrained part of NLPModels soon."}]}]},dir:"\u002Fblog",path:"\u002Fblog\u002F2017-02-07-nlpmodelsjl-cutestjl-and-other-nonlinear-optimization-packages-on-julia",extension:".md",createdAt:E,updatedAt:E,bodyPlainText:"\n# NLPModels.jl, CUTEst.jl and other Nonlinear Optimization Packages on Julia\n\nA couple of weeks ago me and Professor [Dominique Orban](https:\u002F\u002Fdpo.github.io) have finally made a release of\nCUTEst.jl, a wrapper for the CUTEst repository of problems for nonlinear\noptimization (which I've mentioned before).\nAlong with this release, we've done a release of NLPModels.jl, the underlying\npackage. I think it's time I explain a little about these packages, others,\nand how to use them together.\nIf you want to see the output of the commands, you can open\n[this ASCIInema](https:\u002F\u002Fasciinema.org\u002Fa\u002F102371)\nside by side.\n\n_Obs.: Tutorial using Julia 0.5.0_\n\n_Edit: Second part is\n[here](https:\u002F\u002Fabelsiqueira.github.io{{local_prefix}}nlpmodelsjl-and-cutestjl-constrained-optimization\u002F)._\n\n**JuliaSmoothOptimizers**\n[![JuliaSmoothOptimizers logo](https:\u002F\u002Fjuliasmoothoptimizers.github.io\u002Fassets\u002Flogo.png){: .img-view }](https:\u002F\u002Fjuliasmoothoptimizers.github.io)\n\nMost packages mentioned here will be a part of the JuliaSmoothOptimizers (JSO)\norganization. There are more packages in the organization that I won't mention here, but you should check it out.\n\n**NLPModels.jl**\n\nNLPModels is a package for creating Nonlinear Optimization Models. It is\nfocused on the needs of the solver writer, such as the ability to write one\nsolver that works on many models.\nThe package defines a few models, and there are others on the horizon.\nThe ones already done are:\n\n- **ADNLPModel**: A model with automatic differentiation;\n- **MathProgNLPModel**: A model for [MathProgBase](https:\u002F\u002Fgithub.com\u002FJuliaOpt\u002FMathProgBase.jl)\u002F[JuMP](http:\u002F\u002Fgithub.com\u002FJuliaOpt\u002FJuMP.jl) conversion, whose utility will be shown below (obs: MPB and JuMP are packages from the JuliaOpt organization);\n- **SimpleNLPModel**: A model in which nothing is automatic, i.e., all functions have to be provided by the user.\n- **SlackModel**: A model that changes all inequalities to equalities adding extra variables;\n- **LBFGSModel** and **LSR1Model**: Models that create quasi-Newton models from another model.\n\nThe first two models are designed to be easy to use; the third is useful for\nefficient model creation in specific cases; the last ones are utility models.\n\nLet's begin by installing NLPModels.jl, and a couple of optional requirements.\n\n```\nPkg.add(\"NLPModels.jl\")\nPkg.add(\"JuMP.jl\") # Installs ForwardDiff also.\n```\n\nThis should install version 0.1.0. After that, just do\n\n```\nusing NLPModels\n```\n\nNow, let's create a simple function: Rosenbrock's.\n\n```\nf(x) = (x[1] - 1)^2 + 100*(x[2] - x[1]^2)^2\n```\n\nThe Rosenbrock problem traditionally starts from $(-1.2,1.0)$.\n\n```\nx0 = [-1.2; 1.0]\n```\n\nNow, we are ready to create the problem.\n\n```\nadnlp = ADNLPModel(f, x0)\n```\n\nNow, we can access the function and derivatives using the [NLPModels API](https:\u002F\u002Fjuliasmoothoptimizers.github.io\u002FNLPModels.jl\u002Fstable\u002Fapi.html)\n\n```\nobj(adnlp, adnlp.meta.x0)\ngrad(adnlp, adnlp.meta.x0)\nhess(adnlp, adnlp.meta.x0)\nobjgrad(adnlp, adnlp.meta.x0)\nhprod(adnlp, adnlp.meta.x0, ones(2))\nH = hess_op(adnlp, adnlp.meta.x0)\nH * ones(2)\n```\n\nAt this point, we can't differentiate many things from simply using\n`ForwardDiff` interface directly, but two things stand out: `objgrad` returns\nboth functions at once, and `hess_op` returns a\n[LinearOperator](https:\u002F\u002Fgithub.com\u002FJuliaSmoothOptimizers\u002FLinearOperators.jl),\nanother structure created in JuliaSmoothOptimizers.\nThis one defines a linear operator, extending Julia matrices in the sense that if\n\n```\nusing LinearOperators\nn = 100\nA = rand(n, n)\nB = rand(n, n)\nopA = LinearOperator(A)\nopB = LinearOperator(B)\nv = rand(n)\n```\n\nthen `(A * B) * v` computes the matrix product, whereas `(opA * opB) * v` won't.\nFurthermore, the linear operator can be created from the functions\n`v-\u003EMp(v)` and `v-\u003EMtp(v)`, defining the product of the linear operator times a vector and its transpose times a vector.\n\n```\nT = LinearOperator(2, 2, # sizes\n                   false, false,\n                   v-\u003E[-v[2]; v[1]], v-\u003E[v[2]; -v[1]])\nv = rand(2)\nT * v\nT' * v\n```\n\n_Obs: In the `ADNLPModel` case, `hess_op` returns a linear operator that is actually\ncomputing the matrix, but this is a issue to be tackled on the future (PRs\nwelcome). But we'll be back with uses for `hess_op` soon._\n\nThe next model is the `MathProgNLPModel`. This model's main use is the `JuMP`\nmodelling language. This is very useful for more elaborate writing, specially\nwith constraints. It does create a little more overhead though, so keep that\nin mind.\n\n```\nusing JuMP\njmp = Model()\n@variable(jmp, x[i=1:2], start=(x0[i])) # x0 from before\n@NLobjective(jmp, Min, (x[1] - 1)^2 + 100*(x[2] - x[1]^2)^2)\nmpbnlp = MathProgNLPModel(jmp)\n```\n\nTry the commands again.\n\n```\nobj(mpbnlp, mpbnlp.meta.x0)\ngrad(mpbnlp, mpbnlp.meta.x0)\nhess(mpbnlp, mpbnlp.meta.x0)\nobjgrad(mpbnlp, mpbnlp.meta.x0)\nhprod(mpbnlp, mpbnlp.meta.x0, ones(2))\nH = hess_op(mpbnlp, mpbnlp.meta.x0)\nH * ones(2)\n```\n\nIt should be pretty much the same, though there is a little difference in `hess`.\nJuMP creates the sparse Hessian, which is better, from a computational point of\nview.\n\nNotice how the commands are the same. I've actually copy-pasted the commands\nfrom above.\nThis allows the write of a solver in just a couple of commands.\nFor instance, a simple **Newton method**.\n\n```\nfunction newton(nlp :: AbstractNLPModel)\n  x = nlp.meta.x0\n  fx = obj(nlp, x)\n  gx = grad(nlp, x)\n  ngx = norm(gx)\n  while ngx \u003E 1e-6\n    Hx = hess(nlp, x)\n    d = -gx\n    try\n      G = chol(Hermitian(Hx, :L)) # Make Cholesky work on lower-only matrix.\n      d = -G\\(G'\\gx)\n    catch e\n      if !isa(e, Base.LinAlg.PosDefException); rethrow(e); end\n    end\n    t = 1.0\n    xt = x + t * d\n    ft = obj(nlp, xt)\n    while ft \u003E fx + 0.5 * t * dot(gx, d)\n      t *= 0.5\n      xt = x + t * d\n      ft = obj(nlp, xt)\n    end\n    x = xt\n    fx = ft\n    gx = grad(nlp, x)\n    ngx = norm(gx)\n  end\n  return x, fx, ngx\nend\n```\n\nAnd we run in the problems with\n\n```\nnewton(adnlp)\nnewton(mpbnlp)\n```\n\n_Write once, use on problems from different sources._\n\nNow, to have more fun, let's get another package:\n[OptimizationProblems.jl](https:\u002F\u002Fgithub.com\u002FJuliaSmoothOptimizers\u002FOptimizationProblems.jl).\nThis package doesn't have a release yet, so we have to clone it:\n\n```\nPkg.clone(\"https:\u002F\u002Fgithub.com\u002FJuliaSmoothOptimizers\u002FOptimizationProblems.jl\")\n```\n\nWhat we have here is a collection of JuMP models implementing some of the\nCUTEst problems. Together with `NLPModels.jl`, we have a good opportunity to test our Newton implementation.\n\n```\nusing OptimizationProblems\n\nx, fx, ngx = newton(MathProgNLPModel( rosenbrock() ))\nx, fx, ngx = newton(MathProgNLPModel( dixmaanj() ))\nx, fx, ngx = newton(MathProgNLPModel( brownbs() ))\n```\n\n_An issue with OptimizationProblems is that it still doesn't have a way to get\nall unconstrained problems, for instance. (PRs are welcome)._\n\nSo far we used 3 packages from JSO: `NLPModels.jl`, `LinearOperators.jl` and `OptimizationProblems.jl`. It's time to meet another important package.\n\n**CUTEst.jl**\n\nCUTEst, the Constrained and Unconstrained Testing Environment with safe\nthreads, is a package written in Fortran providing over a thousand problems to\nallow testing of Nonlinear Programming solvers. However, CUTEst is hard to use\nby first-timers. Just installing it was already hard.\nCUTEst.jl provides an interface for CUTEst that is simple to install and use\n(comparing to the original).\n\n_Obs.: CUTEst.jl does not work on Windows for now. In fact, there is no plan to\nmake it work on Windows because \"people interested in doing it\"∩\"people capable\nof doing it\" = ∅, as far as we've looked. If you are in this set, PRs are\nwelcome._\n\nTo install CUTEst.jl you need to install something manually. Unfortunately,\nthis is specific for each system. Except for OSX, actually, which is using\n[homebrew-cutest](https:\u002F\u002Fgithub.com\u002Foptimizers\u002Fhomebrew-cutest).\n\nFor Linux users, check out [this\npage](http:\u002F\u002Fjuliasmoothoptimizers.github.io\u002FCUTEst.jl\u002Flatest\u002F#Installing-1).\nEssentially, we need `libgfortran.so` in a visible place. And it's especially\nannoying that some distritions don't put it in a visible place.\n\nWith that done, enter\n\n```\nPkg.add(\"CUTEst\")\n```\n\nwhich should install CUTEst.jl 0.1.0.\n\nYes, it takes some time.\n\nFinally, we start using CUTEst with\n\n```\nusing CUTEst\n\nnlp = CUTEstModel(\"ROSENBR\")\n```\n\n`ROSENBR` is a CUTEst problem, in case you want the list, see\n[here](http:\u002F\u002Fwww.cuter.rl.ac.uk\u002FProblems\u002Fmastsif.html). Keep reading for a way\nto select them, though.\n\nNow, let's solve this CUTEst problem with our Newton method.\n\n```\nx, fx, ngx = newton(nlp)\n```\n\n**Yes, exactly like before!**.\n\nCUTEst is a little more annoying in other aspect also. Like, you can't have two\nor more problems open at the same time, and you have to close this problem\nbefore opening a new one. (Again, PRs are welcome).\n\n```\nfinalize(nlp)\nnlp = CUTEstModel(\"HIMMELBB\")\nx, fx, ngx = newton(nlp)\nfinalize(nlp)\n```\n\nThis allows a simple workflow for writing optimization solvers.\n\n- Write some problems by hand (using `ADNLPModel` or `MathProgNLPModel`);\n- Test your solvers with these hand-written problems;\n- Repeat last two steps until you believe you're ready to competitive comparison;\n- Test with CUTEst problems seamlessly.\n\nNow, let's get back to `hess_op`. Remember that it used Matrix vector products?\nWell, CUTEst has separate functions for the product of the Hessian at a point\nand a vector. Which means that `hprod` actually computes this product without\nhaving to create the matrix. Which means it is, at least, memory-efficient.\nFurthermore, `hess_op` will be created with the `hprod` function, which means\nit is also memory-efficient.\n\nLet's look at a huge problem to feel the difference.\n\n```\nnlp = CUTEstModel(\"BOX\")\nnlp.meta.nvar\n```\n\nLet's make a simple comparison\n\n```\nfunction foo1()\n  H = hess(nlp, nlp.meta.x0)\n  v = ones(nlp.meta.nvar)\n  return Hermitian(H, :L) * v\nend\n\nfunction foo2()\n  H = hess_op(nlp, nlp.meta.x0)\n  v = ones(nlp.meta.nvar)\n  return H * v\nend\n\n@time w1 = foo1();\n@time w2 = foo2();\nnorm(w1 - w2)\n```\n\nYes, that's a huge difference.\n\nThis is a very good reason to use `hess_op` and `hprod`. But let's take a step further.\n\nWe can't implement Cholesky using only `hprod`s, so our Newton method would\nactually take a long time to reach a solution for the problem above.\nTo circunvent that, we could try using the Conjugate Gradients Method instead\nof Cholesky. This would only use Hessian-vector products.\n\nWe arrive on a new package,\n[Krylov.jl](https:\u002F\u002Fgithub.com\u002FJuliaSmoothOptimizers\u002FKrylov.jl), which\nimplements Krylov methods. In particular, Conjugate Gradients.\nThis package is also unreleased, so we need to clone it.\n\n```\nPkg.clone(\"https:\u002F\u002Fgithub.com\u002FJuliaSmoothOptimizers\u002FKrylov.jl\")\n```\n\nConsider a simple example\n\n```\nusing Krylov\nA = rand(3,3)\nA = A*A'\nb = A*ones(3)\ncg(A, b)\n```\n\nAs expected, the system is solver, and the solution is $(1,1,1)$.\nBut let's do something more.\n\n```\nA = -A\ncg(A, b)\n```\n\nYes, Krylov does indeed solves the non-positive definite system using Conjugate Gradient.\nWell, actually, a variant.\n\nThat's not enough tough. Krylov.jl also accepts an additional argument `radius`\nto set a trust-region radius.\n\n```\ncg(A, b, radius=0.1)\n```\n\nWell, as an exercise I suggest you implement a trust-region version of Newton\nmethod, but for now, let's continue with our line-search version.\n\nWe know now how `cg` behaves for non-positive definite systems, we can't make\nthe changes for a new method.\n\n```\nfunction newton2(nlp :: AbstractNLPModel)\n  x = nlp.meta.x0\n  fx = obj(nlp, x)\n  gx = grad(nlp, x)\n  ngx = norm(gx)\n  while norm(gx) \u003E 1e-6\n    Hx = hess_op(nlp, x)\n    d, _ = cg(Hx, -gx)\n    slope = dot(gx, d)\n    if slope \u003E= 0 # Not a descent direction\n      d = -gx\n      slope = -dot(d,d)\n    end\n    t = 1.0\n    xt = x + t * d\n    ft = obj(nlp, xt)\n    while ft \u003E fx + 0.5 * t * slope\n      t *= 0.5\n      xt = x + t * d\n      ft = obj(nlp, xt)\n    end\n    x = xt\n    fx = ft\n    gx = grad(nlp, x)\n    ngx = norm(gx)\n  end\n  return x, fx, ngx\nend\n```\n\nNow, running `newton2` on our large problem, we obtain\n\n```\nx, fx, ngx = newton2(nlp)\n```\n\nWhich is the method working very fast. Less that a second here.\n\n---\n\nThere is actually another package I'd like to talk about, but it needs some\nmore work for it to be ready for a release:\n\n**Optimize.jl**\n\nOptimize.jl is a package with solvers. We intend to implement some high quality\nsolvers in there, but there is actually more to it. We have in there tools to\nbenchmark packages. These tools should allow the testing of a set of solvers in\na set of problems without much fuss, while creating the comparison information,\nincluding the performance profile.\nIt also includes, or will include, \"parts\" of solvers to create your own\nsolver. Like trust-region and line-search algorithms and auxiliary functions\nand types.\nUnfortunately, it's not done enough for me to extend on it, and this is already\ngetting too long.\n\n**End**\n\nI hope you enjoyed this overview of packages.\nSubscribe to the RSS feed to keep updated in future tutorials. I intend to talk\nabout the constrained part of NLPModels soon.\n"},prev:{slug:"2017-02-17-nlpmodelsjl-and-cutestjl-constrained-optimization",title:"NLPModels.jl and CUTEst.jl: Constrained optimization"},next:{slug:"2017-01-15-julia-fractal-on-julia",title:"Julia Fractal on Julia"}}],fetch:{},mutations:void 0}}("text","element","\n","p","code","div","nuxt-content-highlight","pre","language-text","line-numbers","a","nofollow","noopener","noreferrer","_blank","strong","li","em","hess_op","."," and ","hprod","NLPModels.jl","ADNLPModel","MathProgNLPModel","NLPModels.jl, CUTEst.jl and other Nonlinear Optimization Packages on Julia","here","ul","JuMP","OptimizationProblems.jl","2022-12-23T23:04:30.335Z")));