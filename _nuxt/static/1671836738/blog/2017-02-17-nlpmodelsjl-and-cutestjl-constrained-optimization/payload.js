__NUXT_JSONP__("/blog/2017-02-17-nlpmodelsjl-and-cutestjl-constrained-optimization", (function(a,b,c,d,e,f,g,h,i,j,k,l,m,n,o,p,q,r,s,t,u,v,w,x,y,z,A,B,C,D,E,F,G,H,I,J,K,L){return {data:[{page:{slug:"2017-02-17-nlpmodelsjl-and-cutestjl-constrained-optimization",title:B,date:"2017-02-17",tags:["julia","nlpmodels","cutest","work","optimization","constrained"],toc:[],body:{type:"root",children:[{type:b,tag:"h1",props:{id:"nlpmodelsjl-and-cutestjl-constrained-optimization"},children:[{type:b,tag:q,props:{href:"#nlpmodelsjl-and-cutestjl-constrained-optimization",ariaHidden:s,tabIndex:-1},children:[{type:b,tag:"span",props:{className:["icon","icon-link"]},children:[]}]},{type:a,value:B}]},{type:a,value:c},{type:b,tag:e,props:{},children:[{type:a,value:"This is a continuation of "},{type:b,tag:q,props:{href:"https:\u002F\u002Fabelsiqueira.github.io%7B%7Blocal_prefix%7D%7Dnlpmodelsjl-cutestjl-and-other-nonlinear-optimization-packages-on-julia\u002F",rel:[t,u,v],target:w},children:[{type:a,value:"this\npost"}]},{type:a,value:".\nAnd again, you can follow the commands of this post in the\n"},{type:b,tag:q,props:{href:"https:\u002F\u002Fasciinema.org\u002Fa\u002F103654",rel:[t,u,v],target:w},children:[{type:a,value:"asciinema"}]},{type:a,value:l}]},{type:a,value:c},{type:b,tag:e,props:{},children:[{type:a,value:"If you followed along last post, you should know the basics of our\nNLPModels API, including CUTEst access."}]},{type:a,value:c},{type:b,tag:e,props:{},children:[{type:a,value:"One thing I didn't explore, though, was constrained problems.\nIt'd complicate too much."}]},{type:a,value:c},{type:b,tag:e,props:{},children:[{type:a,value:"However, now that we know how to handle the basics, we can move to the\nadvanced."}]},{type:a,value:c},{type:b,tag:e,props:{},children:[{type:b,tag:m,props:{},children:[{type:a,value:"Nonlinear Programming format"}]}]},{type:a,value:c},{type:b,tag:e,props:{},children:[{type:a,value:"The NLPModels internal structure is based on the CUTEst way of storing a\nproblem.\nWe use the following form for the optimization problem:"}]},{type:a,value:c},{type:b,tag:e,props:{},children:[{type:a,value:"$$\n\\begin{align}\n\\min \\quad & f(x) \\\ns.t. \\quad & c_L \\leq c(x) \\leq c_U \\\n& \\ell \\leq x \\leq u\\end{align}\n$$"}]},{type:a,value:c},{type:b,tag:e,props:{},children:[{type:a,value:"Given an "},{type:b,tag:d,props:{},children:[{type:a,value:"AbstractNLPModel"}]},{type:a,value:" named "},{type:b,tag:d,props:{},children:[{type:a,value:"nlp"}]},{type:a,value:", the values for $\\ell$, $u$, $c_L$ and\n$c_U$ are stored in an "},{type:b,tag:d,props:{},children:[{type:a,value:"NLPModelMeta"}]},{type:a,value:" structure, and can be accessed by\nthrough "},{type:b,tag:d,props:{},children:[{type:a,value:"nlp.meta"}]},{type:a,value:l}]},{type:a,value:c},{type:b,tag:e,props:{},children:[{type:a,value:"Let's look back at the simple Rosenbrock problem of before."}]},{type:a,value:c},{type:b,tag:g,props:{className:[h]},children:[{type:b,tag:i,props:{className:[j,k]},children:[{type:b,tag:d,props:{},children:[{type:a,value:"using NLPModels\n\nf(x) = (x[1] - 1)^2 + 100*(x[2] - x[1]^2)^2\nx0 = [-1.2; 1.0]\nnlp = ADNLPModel(f, x0)\nprint(nlp.meta)\n"}]}]}]},{type:a,value:c},{type:b,tag:e,props:{},children:[{type:a,value:"You should be seeing this:"}]},{type:a,value:c},{type:b,tag:g,props:{className:[h]},children:[{type:b,tag:i,props:{className:[j,k]},children:[{type:b,tag:d,props:{},children:[{type:a,value:"Minimization problem Generic\nnvar = 2, ncon = 0 (0 linear)\nlvar = -Inf  -Inf\nuvar = Inf  Inf\nlcon = ∅\nucon = ∅\nx0 = -1.2  1.0\ny0 = ∅\nnnzh = 4\nnnzj = 0\n"}]}]}]},{type:a,value:c},{type:b,tag:e,props:{},children:[{type:a,value:"Although the meaning of these values is reasonably straigthforward, I'll explain a bit."}]},{type:a,value:c},{type:b,tag:r,props:{},children:[{type:a,value:c},{type:b,tag:f,props:{},children:[{type:b,tag:d,props:{},children:[{type:a,value:"nvar"}]},{type:a,value:" is the number of variables in a problem;"}]},{type:a,value:c},{type:b,tag:f,props:{},children:[{type:b,tag:d,props:{},children:[{type:a,value:"ncon"}]},{type:a,value:" is the number of constraints, without counting the bounds;"}]},{type:a,value:c},{type:b,tag:f,props:{},children:[{type:b,tag:d,props:{},children:[{type:a,value:"lvar"}]},{type:a,value:" is the vector $\\ell$, the lower bounds on the variables;"}]},{type:a,value:c},{type:b,tag:f,props:{},children:[{type:b,tag:d,props:{},children:[{type:a,value:"uvar"}]},{type:a,value:" is the vector $u$, the upper bounds on the variables;"}]},{type:a,value:c},{type:b,tag:f,props:{},children:[{type:b,tag:d,props:{},children:[{type:a,value:x}]},{type:a,value:" is the vector $c_L$, the lower bounds of the constraints function;"}]},{type:a,value:c},{type:b,tag:f,props:{},children:[{type:b,tag:d,props:{},children:[{type:a,value:y}]},{type:a,value:" is the vector $c_U$, the upper bounds of the constraints function;"}]},{type:a,value:c},{type:b,tag:f,props:{},children:[{type:b,tag:d,props:{},children:[{type:a,value:"x0"}]},{type:a,value:" is the initial approximation to the solution, aka the starting point;"}]},{type:a,value:c},{type:b,tag:f,props:{},children:[{type:b,tag:d,props:{},children:[{type:a,value:"y0"}]},{type:a,value:" is the initial approximation to the Lagrange multipliers;"}]},{type:a,value:c},{type:b,tag:f,props:{},children:[{type:b,tag:d,props:{},children:[{type:a,value:z}]},{type:a,value:" is the number of nonzeros on the Hessian¹;"}]},{type:a,value:c},{type:b,tag:f,props:{},children:[{type:b,tag:d,props:{},children:[{type:a,value:C}]},{type:a,value:" is the number of nonzeros on the Jacobian¹;"}]},{type:a,value:c}]},{type:a,value:c},{type:b,tag:e,props:{},children:[{type:b,tag:D,props:{},children:[{type:a,value:"¹ "},{type:b,tag:d,props:{},children:[{type:a,value:z}]},{type:a,value:n},{type:b,tag:d,props:{},children:[{type:a,value:C}]},{type:a,value:" are not consistent between models, because some consider the dense matrix, and for the Hessian, some consider only the triangle. However, if you're possibly considering using "},{type:b,tag:d,props:{},children:[{type:a,value:z}]},{type:a,value:", you're probably looking for "},{type:b,tag:d,props:{},children:[{type:a,value:E}]},{type:a,value:" too, and "},{type:b,tag:d,props:{},children:[{type:a,value:E}]},{type:a,value:" returns with the correct size."}]}]},{type:a,value:c},{type:b,tag:e,props:{},children:[{type:a,value:"These values can be accessed directly as fields in "},{type:b,tag:d,props:{},children:[{type:a,value:A}]},{type:a,value:" with the same name above."}]},{type:a,value:c},{type:b,tag:g,props:{className:[h]},children:[{type:b,tag:i,props:{className:[j,k]},children:[{type:b,tag:d,props:{},children:[{type:a,value:"nlp.meta.ncon\nnlp.meta.x0\nnlp.meta.lvar\n"}]}]}]},{type:a,value:c},{type:b,tag:e,props:{},children:[{type:b,tag:m,props:{},children:[{type:a,value:"Bounds"}]}]},{type:a,value:c},{type:b,tag:e,props:{},children:[{type:a,value:"Now, let's create a bounded problem."}]},{type:a,value:c},{type:b,tag:g,props:{className:[h]},children:[{type:b,tag:i,props:{className:[j,k]},children:[{type:b,tag:d,props:{},children:[{type:a,value:"nlp = ADNLPModel(f, x0, lvar=zeros(2), uvar=[0.4; 0.6])\nprint(nlp.meta)\n"}]}]}]},{type:a,value:c},{type:b,tag:e,props:{},children:[{type:a,value:"Now the bounds are set, and you can access them with"}]},{type:a,value:c},{type:b,tag:g,props:{className:[h]},children:[{type:b,tag:i,props:{className:[j,k]},children:[{type:b,tag:d,props:{},children:[{type:a,value:"nlp.meta.lvar\nnlp.meta.uvar\n"}]}]}]},{type:a,value:c},{type:b,tag:e,props:{},children:[{type:a,value:"That's pretty much it. For "},{type:b,tag:d,props:{},children:[{type:a,value:"SimpleNLPModel"}]},{type:a,value:", it's the same thing.\n"},{type:b,tag:d,props:{},children:[{type:a,value:"MathProgNLPModel"}]},{type:a,value:" inherits the bounds, as expected:"}]},{type:a,value:c},{type:b,tag:g,props:{className:[h]},children:[{type:b,tag:i,props:{className:[j,k]},children:[{type:b,tag:d,props:{},children:[{type:a,value:"using JuMP\n\njmp = Model()\nu = [0.4; 0.6]\n@variable(jmp, 0 \u003C= x[i=1:2] \u003C= u[i], start=(x0[i]))\n@NLobjective(jmp, Min, (x[1] - 1)^2 + 100*(x[2] - x[1]^2)^2)\nmpbnlp = MathProgNLPModel(jmp)\nprint(mpbnlp.meta)\n"}]}]}]},{type:a,value:c},{type:b,tag:e,props:{},children:[{type:a,value:"For CUTEst, there is no differentiation on creating a problem with bounds or\nnot, since it uses the internal description of the problem.\nFor instance, "},{type:b,tag:d,props:{},children:[{type:a,value:"HS4"}]},{type:a,value:" is a bounded problem."}]},{type:a,value:c},{type:b,tag:g,props:{className:[h]},children:[{type:b,tag:i,props:{className:[j,k]},children:[{type:b,tag:d,props:{},children:[{type:a,value:"using CUTEst\n\nclp = CUTEstModel(\"HS4\")\nprint(clp.meta)\nfinalize(clp)\n"}]}]}]},{type:a,value:c},{type:b,tag:e,props:{},children:[{type:a,value:"Notice that it can happen that one or more of the variables is unlimited\n(lower, upper or both). This is represented by the value "},{type:b,tag:d,props:{},children:[{type:a,value:F}]},{type:a,value:" in Julia.\nThis should be expected since the unconstrained problem already used these\n"},{type:b,tag:d,props:{},children:[{type:a,value:F}]},{type:a,value:" values."}]},{type:a,value:c},{type:b,tag:e,props:{},children:[{type:a,value:"On the other hand, it could happen that $\\ell_i = u_i$, in which case the\nvariable is fixed, or that $\\ell_i \u003E u_i$, in which case the variable (and the\nproblem) is infeasible.\nNote that "},{type:b,tag:d,props:{},children:[{type:a,value:"NLPModels"}]},{type:a,value:" only creates the model, it doesn't check whether it is\nfeasible or not, even in this simple example. That said, CUTEst shouldn't have\nany infeasible variable."}]},{type:a,value:c},{type:b,tag:e,props:{},children:[{type:a,value:"Furthermore, all these types of bounds can be accessed from "},{type:b,tag:d,props:{},children:[{type:a,value:A}]},{type:a,value:". Notice that\nthere are 6 possible situations:"}]},{type:a,value:c},{type:b,tag:r,props:{},children:[{type:a,value:c},{type:b,tag:f,props:{},children:[{type:a,value:"Free variables, stored in "},{type:b,tag:d,props:{},children:[{type:a,value:"meta.ifree"}]},{type:a,value:o}]},{type:a,value:c},{type:b,tag:f,props:{},children:[{type:a,value:"Fixed variables, stored in "},{type:b,tag:d,props:{},children:[{type:a,value:"meta.ifix"}]},{type:a,value:o}]},{type:a,value:c},{type:b,tag:f,props:{},children:[{type:a,value:"Variables bounded below, stored in "},{type:b,tag:d,props:{},children:[{type:a,value:"meta.ilow"}]},{type:a,value:o}]},{type:a,value:c},{type:b,tag:f,props:{},children:[{type:a,value:"Variables bounded above, stored in "},{type:b,tag:d,props:{},children:[{type:a,value:"meta.iupp"}]},{type:a,value:o}]},{type:a,value:c},{type:b,tag:f,props:{},children:[{type:a,value:"Variables bounded above and below, stored in "},{type:b,tag:d,props:{},children:[{type:a,value:"meta.irng"}]},{type:a,value:o}]},{type:a,value:c},{type:b,tag:f,props:{},children:[{type:a,value:"Infeasible variables, stored in "},{type:b,tag:d,props:{},children:[{type:a,value:"meta.iinf"}]},{type:a,value:l}]},{type:a,value:c}]},{type:a,value:c},{type:b,tag:e,props:{},children:[{type:a,value:"Here is one example with one of each of them"}]},{type:a,value:c},{type:b,tag:g,props:{className:[h]},children:[{type:b,tag:i,props:{className:[j,k]},children:[{type:b,tag:d,props:{},children:[{type:a,value:"nlp = ADNLPModel(x-\u003Edot(x,x), zeros(6),\n  lvar = [-Inf, -Inf, 0.0, 0.0, 0.0,  0.0],\n  uvar = [ Inf,  1.0, Inf, 1.0, 0.0, -1.0])\nnlp.meta.ifree\nnlp.meta.ifix\nnlp.meta.ilow\nnlp.meta.iupp\nnlp.meta.irng\nnlp.meta.iinf\n"}]}]}]},{type:a,value:c},{type:b,tag:e,props:{},children:[{type:b,tag:m,props:{},children:[{type:a,value:"Constraints"}]}]},{type:a,value:c},{type:b,tag:e,props:{},children:[{type:a,value:"Constraints are stored in NLPModels following in the format $c_L \\leq c(x) \\leq c_U$.\nThat means that an equality constraint happens when $c_{L_j} = c_{U_j}$.\nLet's look at how to create a problem with constraints."}]},{type:a,value:c},{type:b,tag:e,props:{},children:[{type:a,value:"For "},{type:b,tag:d,props:{},children:[{type:a,value:"ADNLPModel"}]},{type:a,value:", you need to pass three keywords arguments: "},{type:b,tag:d,props:{},children:[{type:a,value:G}]},{type:a,value:p},{type:b,tag:d,props:{},children:[{type:a,value:x}]},{type:a,value:n},{type:b,tag:d,props:{},children:[{type:a,value:y}]},{type:a,value:",\nwhich represent $c(x)$, $c_L$ and $c_U$, respectively.\nFor instance, the problem"}]},{type:a,value:c},{type:b,tag:e,props:{},children:[{type:a,value:"$$\n\\begin{align}\n\\min \\quad & x_1^2 + x_2^2 \\\ns.t. \\quad & x_1 + x_2 = 1\n\\end{align}\n$$"}]},{type:a,value:c},{type:b,tag:e,props:{},children:[{type:a,value:"is created by doing"}]},{type:a,value:c},{type:b,tag:g,props:{className:[h]},children:[{type:b,tag:i,props:{className:[j,k]},children:[{type:b,tag:d,props:{},children:[{type:a,value:"c(x) = [x[1] + x[2] - 1]\nlcon = [0.0]\nucon = [0.0]\nnlp = ADNLPModel(x-\u003Edot(x,x), zeros(2), c=c, lcon=lcon, ucon=ucon)\n"}]}]}]},{type:a,value:c},{type:b,tag:e,props:{},children:[{type:a,value:"or alternatively, if you don't want the intermediary functions"}]},{type:a,value:c},{type:b,tag:g,props:{className:[h]},children:[{type:b,tag:i,props:{className:[j,k]},children:[{type:b,tag:d,props:{},children:[{type:a,value:"nlp = ADNLPModel(x-\u003Edot(x,x), zeros(2), c=x-\u003E[x[1]+x[2]-1], lcon=[0.0], ucon=[0.0])\n"}]}]}]},{type:a,value:c},{type:b,tag:e,props:{},children:[{type:a,value:"Another possibility is to do"}]},{type:a,value:c},{type:b,tag:g,props:{className:[h]},children:[{type:b,tag:i,props:{className:[j,k]},children:[{type:b,tag:d,props:{},children:[{type:a,value:"nlp = ADNLPModel(x-\u003Edot(x,x), zeros(2), c=x-\u003E[x[1]+x[2]], lcon=[1.0], ucon=[1.0])\n"}]}]}]},{type:a,value:c},{type:b,tag:e,props:{},children:[{type:a,value:"Personally, I prefer the former."}]},{type:a,value:c},{type:b,tag:e,props:{},children:[{type:a,value:"For inequalities, you can have only lower, only upper, and both.\nThe commands"}]},{type:a,value:c},{type:b,tag:g,props:{className:[h]},children:[{type:b,tag:i,props:{className:[j,k]},children:[{type:b,tag:d,props:{},children:[{type:a,value:"nlp = ADNLPModel(x-\u003Edot(x,x), zeros(2),\n  c=x-\u003E[x[1] + x[2]; 3x[1] + 2x[2]; x[1]*x[2]],\n  lcon = [-1.0; -Inf; 1.0],\n  ucon = [Inf;   3.0; 2.0])\n"}]}]}]},{type:a,value:c},{type:b,tag:e,props:{},children:[{type:a,value:"implement the problem"}]},{type:a,value:c},{type:b,tag:e,props:{},children:[{type:a,value:"$$\n\\begin{align}\n\\min \\quad & x_1^2 + x_2^2 \\\ns.t. \\quad & x_1 + x_2 \\geq -1 \\\n& 3x_1 + 2x_2 \\leq 3 \\\n& 1 \\leq x_1x_2 \\leq 2.\n\\end{align}\n$$"}]},{type:a,value:c},{type:b,tag:e,props:{},children:[{type:a,value:"Again, the types of constraints can be accessed in "},{type:b,tag:d,props:{},children:[{type:a,value:A}]},{type:a,value:", through\n"},{type:b,tag:d,props:{},children:[{type:a,value:"nlp.meta.jfix"}]},{type:a,value:p},{type:b,tag:d,props:{},children:[{type:a,value:"jfree"}]},{type:a,value:p},{type:b,tag:d,props:{},children:[{type:a,value:"jinf"}]},{type:a,value:p},{type:b,tag:d,props:{},children:[{type:a,value:"jlow"}]},{type:a,value:p},{type:b,tag:d,props:{},children:[{type:a,value:"jrng"}]},{type:a,value:n},{type:b,tag:d,props:{},children:[{type:a,value:"jupp"}]},{type:a,value:".\nNotice if you forget to set "},{type:b,tag:d,props:{},children:[{type:a,value:x}]},{type:a,value:n},{type:b,tag:d,props:{},children:[{type:a,value:y}]},{type:a,value:", there will be no\nconstraints, even though "},{type:b,tag:d,props:{},children:[{type:a,value:G}]},{type:a,value:" is set. This is because the number of\nconstraints is taken from the lenght of these vectors."}]},{type:a,value:c},{type:b,tag:e,props:{},children:[{type:a,value:"Now, to access these constraints, let's consider this simple problem."}]},{type:a,value:c},{type:b,tag:g,props:{className:[h]},children:[{type:b,tag:i,props:{className:[j,k]},children:[{type:b,tag:d,props:{},children:[{type:a,value:"nlp = ADNLPModel(f, x0, c=x-\u003E[x[1]*x[2] - 0.5], lcon=[0.0], ucon=[0.0])\n"}]}]}]},{type:a,value:c},{type:b,tag:e,props:{},children:[{type:a,value:H},{type:b,tag:d,props:{},children:[{type:a,value:"cons"}]},{type:a,value:" return $c(x)$."}]},{type:a,value:c},{type:b,tag:g,props:{className:[h]},children:[{type:b,tag:i,props:{className:[j,k]},children:[{type:b,tag:d,props:{},children:[{type:a,value:"cons(nlp, nlp.meta.x0)\n"}]}]}]},{type:a,value:c},{type:b,tag:e,props:{},children:[{type:a,value:H},{type:b,tag:d,props:{},children:[{type:a,value:"jac"}]},{type:a,value:" returns the Jacobian of $c$. "},{type:b,tag:d,props:{},children:[{type:a,value:"jprod"}]},{type:a,value:n},{type:b,tag:d,props:{},children:[{type:a,value:"jtprod"}]},{type:a,value:" the\nJacobian product times a vector, and "},{type:b,tag:d,props:{},children:[{type:a,value:"jac_op"}]},{type:a,value:" the LinearOperator."}]},{type:a,value:c},{type:b,tag:g,props:{className:[h]},children:[{type:b,tag:i,props:{className:[j,k]},children:[{type:b,tag:d,props:{},children:[{type:a,value:"jac(nlp, nlp.meta.x0)\njprod(nlp, nlp.meta.x0, ones(2))\njtprod(nlp, nlp.meta.x0, ones(1))\nJ = jac_op(nlp, nlp.meta.x0)\nJ * ones(2)\nJ' * ones(1)\n"}]}]}]},{type:a,value:c},{type:b,tag:e,props:{},children:[{type:a,value:"To get the Hessian we'll use the same functions as the unconstrained case,\nwith the addition of a keyword parameter "},{type:b,tag:d,props:{},children:[{type:a,value:"y"}]},{type:a,value:l}]},{type:a,value:c},{type:b,tag:g,props:{className:[h]},children:[{type:b,tag:i,props:{className:[j,k]},children:[{type:b,tag:d,props:{},children:[{type:a,value:"y = [1e4]\nhess(nlp, nlp.meta.x0, y=y)\nhprod(nlp, nlp.meat.x0, ones(2))\nH = hess_op(nlp, nlp.meta.x0, y=y)\nH * ones(2)\n"}]}]}]},{type:a,value:c},{type:b,tag:e,props:{},children:[{type:a,value:"If you want to ignore the objective function, or scale it by some value,\nyou can use the keyword parameter "},{type:b,tag:d,props:{},children:[{type:a,value:"obj_weight"}]},{type:a,value:l}]},{type:a,value:c},{type:b,tag:g,props:{className:[h]},children:[{type:b,tag:i,props:{className:[j,k]},children:[{type:b,tag:d,props:{},children:[{type:a,value:"s = 0.0\nhess(nlp, nlp.meta.x0, y=y, obj_weight=s)\nhprod(nlp, nlp.meat.x0, ones(2), obj_weight=s)\nH = hess_op(nlp, nlp.meta.x0, y=y, obj_weight=s)\nH * ones(2)\n"}]}]}]},{type:a,value:c},{type:b,tag:e,props:{},children:[{type:a,value:"Check the\n"},{type:b,tag:q,props:{href:"http:\u002F\u002Fjuliasmoothoptimizers.github.io\u002FNLPModels.jl\u002Fstable\u002Fapi.html",rel:[t,u,v],target:w},children:[{type:a,value:"API"}]},{type:a,value:"\nfor more details."}]},{type:a,value:c},{type:b,tag:e,props:{},children:[{type:a,value:"We can also create a constrained JuMP model."}]},{type:a,value:c},{type:b,tag:g,props:{className:[h]},children:[{type:b,tag:i,props:{className:[j,k]},children:[{type:b,tag:d,props:{},children:[{type:a,value:"x0 = [-1.2; 1.0]\njmp = Model()\n@variable(jmp, x[i=1:2], start=(x0[i]))\n@NLobjective(jmp, Min, (x[1] - 1)^2 + 100*(x[2] - x[1]^2)^2)\n@NLcontraint(jmp, x[1]*x[2] == 0.5)\nmpbnlp = MathProgNLPModel(jmp)\ncons(mpbnlp, mpbnlp.meta.x0)\njac(mpbnlp, mpbnlp.meta.x0)\nhess(mpbnlp, mpbnlp.meta.x0, y=y)\n"}]}]}]},{type:a,value:c},{type:b,tag:e,props:{},children:[{type:a,value:"And again, the access in CUTEst problems is the same."}]},{type:a,value:c},{type:b,tag:g,props:{className:[h]},children:[{type:b,tag:i,props:{className:[j,k]},children:[{type:b,tag:d,props:{},children:[{type:a,value:"clp = CUTEstModel(\"BT1\")\ncons(clp, clp.meta.x0)\njac(clp, clp.meta.x0)\nhess(clp, clp.meta.x0, y=clp.meta.y0)\nfinalize(clp)\n"}]}]}]},{type:a,value:c},{type:b,tag:e,props:{},children:[{type:b,tag:m,props:{},children:[{type:a,value:"Convenience functions"}]}]},{type:a,value:c},{type:b,tag:e,props:{},children:[{type:a,value:"There are some convenience functions to check whether a problem has only\nequalities, only bounds, etc.\nFor clarification, we're gonna say function constraint to refer to constraints that are not bounds."}]},{type:a,value:c},{type:b,tag:r,props:{},children:[{type:a,value:c},{type:b,tag:f,props:{},children:[{type:b,tag:d,props:{},children:[{type:a,value:I}]},{type:a,value:J},{type:b,tag:d,props:{},children:[{type:a,value:s}]},{type:a,value:" is variable has bounds."}]},{type:a,value:c},{type:b,tag:f,props:{},children:[{type:b,tag:d,props:{},children:[{type:a,value:K}]},{type:a,value:J},{type:b,tag:d,props:{},children:[{type:a,value:s}]},{type:a,value:" if "},{type:b,tag:d,props:{},children:[{type:a,value:I}]},{type:a,value:" and no function\nconstraints;"}]},{type:a,value:c},{type:b,tag:f,props:{},children:[{type:b,tag:d,props:{},children:[{type:a,value:"unconstrained"}]},{type:a,value:": No function constraints nor bounds;"}]},{type:a,value:c},{type:b,tag:f,props:{},children:[{type:b,tag:d,props:{},children:[{type:a,value:"linearly_constrained"}]},{type:a,value:": There are function constraints, and they are\nlinear; "},{type:b,tag:D,props:{},children:[{type:a,value:"obs: even though a "},{type:b,tag:d,props:{},children:[{type:a,value:K}]},{type:a,value:" problem is linearly\nconstrained, this will return false"}]},{type:a,value:l}]},{type:a,value:c},{type:b,tag:f,props:{},children:[{type:b,tag:d,props:{},children:[{type:a,value:"equality_constrained"}]},{type:a,value:": There are function constraints, and they are all equalities;"}]},{type:a,value:c},{type:b,tag:f,props:{},children:[{type:b,tag:d,props:{},children:[{type:a,value:"inequality_constrained"}]},{type:a,value:": There are function constraints, and they are all inequalities;"}]},{type:a,value:c}]},{type:a,value:c},{type:b,tag:e,props:{},children:[{type:b,tag:m,props:{},children:[{type:a,value:"Example solver"}]}]},{type:a,value:c},{type:b,tag:e,props:{},children:[{type:a,value:"Let's implement a \"simple\" solver for constrained optimization.\nOur solver will loosely follow the Byrd-Omojokun implementation of"}]},{type:a,value:c},{type:b,tag:"blockquote",props:{},children:[{type:a,value:c},{type:b,tag:e,props:{},children:[{type:a,value:"M. Lalee, J. Nocedal, and T. Plantenga. "},{type:b,tag:m,props:{},children:[{type:a,value:"On the implementation of an algorithm for large-scale equality constrained optimization"}]},{type:a,value:". SIAM J. Optim., Vol. 8, No. 3, pp. 682-706, 1998."}]},{type:a,value:c}]},{type:a,value:c},{type:b,tag:g,props:{className:[h]},children:[{type:b,tag:i,props:{className:[j,k]},children:[{type:b,tag:d,props:{},children:[{type:a,value:"function solver(nlp :: AbstractNLPModel)\n  if !equality_constrained(nlp)\n    error(\"This solver is for equality constrained problems\")\n  elseif has_bounds(nlp)\n    error(\"Can't handle bounds\")\n  end\n\n  x = nlp.meta.x0\n\n  fx = obj(nlp, x)\n  cx = cons(nlp, x)\n\n  ∇fx = grad(nlp, x)\n  Jx = jac_op(nlp, x)\n\n  λ = cgls(Jx', -∇fx)[1]\n  ∇ℓx = ∇fx + Jx'*λ\n  norm∇ℓx = norm(∇ℓx)\n\n  Δ = max(0.1, min(100.0, 10norm∇ℓx))\n  μ = 1\n  v = zeros(nlp.meta.nvar)\n\n  iter = 0\n  while (norm∇ℓx \u003E 1e-4 || norm(cx) \u003E 1e-4) && (iter \u003C 10000)\n    # Vertical step\n    if norm(cx) \u003E 1e-4\n      v = cg(Jx'*Jx, -Jx'*cx, radius=0.8Δ)[1]\n      Δp = sqrt(Δ^2 - dot(v,v))\n    else\n      fill!(v, 0)\n      Δp = Δ\n    end\n\n    # Horizontal step\n    # Simplified to consider only ∇ℓx = proj(∇f, Nu(A))\n    B = hess_op(nlp, x, y=λ)\n    B∇ℓx = B * ∇ℓx\n    gtBg = dot(∇ℓx, B∇ℓx)\n    gtγ = dot(∇ℓx, ∇fx + B * v)\n    t = if gtBg \u003C= 0\n      norm∇ℓx \u003E 0 ? Δp\u002Fnorm∇ℓx : 0.0\n    else\n      t = min(gtγ\u002FgtBg, Δp\u002Fnorm∇ℓx)\n    end\n\n    d = v - t * ∇ℓx\n\n    # Trial step acceptance\n    xt = x + d\n    ft = obj(nlp, xt)\n    ct = cons(nlp, xt)\n    γ = dot(d, ∇fx) + 0.5*dot(d, B * d)\n    θ = norm(cx) - norm(Jx * d + cx)\n    normλ = norm(λ, Inf)\n    if θ \u003C= 0\n      μ = normλ\n    elseif normλ \u003E γ\u002Fθ\n      μ = min(normλ, 0.1 + γ\u002Fθ)\n    else\n      μ = 0.1 + γ\u002Fθ\n    end\n    Pred = -γ + μ * θ\n    Ared = fx - ft + μ * (norm(cx) - norm(ct))\n\n    ρ = Ared\u002FPred\n    if ρ \u003E 1e-2\n      x .= xt\n      fx = ft\n      cx .= ct\n      ∇fx = grad(nlp, x)\n      Jx = jac_op(nlp, x)\n      λ = cgls(Jx', -∇fx)[1]\n      ∇ℓx = ∇fx + Jx'*λ\n      norm∇ℓx = norm(∇ℓx)\n      if ρ \u003E 0.75 && norm(d) \u003E 0.99Δ\n        Δ *= 2.0\n      end\n    else\n      Δ *= 0.5\n    end\n\n    iter += 1\n  end\n\n  return x, fx, norm∇ℓx, norm(cx)\nend\n"}]}]}]},{type:a,value:c},{type:b,tag:e,props:{},children:[{type:a,value:"Too loosely, in fact."}]},{type:a,value:c},{type:b,tag:r,props:{},children:[{type:a,value:c},{type:b,tag:f,props:{},children:[{type:a,value:"The horizontal step computes only the Cauchy step;"}]},{type:a,value:c},{type:b,tag:f,props:{},children:[{type:a,value:"No special updates;"}]},{type:a,value:c},{type:b,tag:f,props:{},children:[{type:a,value:"No second-order correction;"}]},{type:a,value:c},{type:b,tag:f,props:{},children:[{type:a,value:"No efficient implementation beyond the easy-to-do."}]},{type:a,value:c}]},{type:a,value:c},{type:b,tag:e,props:{},children:[{type:a,value:"To test how good it is, let's run on the Hock-Schittkowski constrained problems."}]},{type:a,value:c},{type:b,tag:g,props:{className:[h]},children:[{type:b,tag:i,props:{className:[j,k]},children:[{type:b,tag:d,props:{},children:[{type:a,value:"function runcutest()\n  problems = filter(x-\u003Econtains(x, \"HS\") && length(x) \u003C= 5, CUTEst.select(only_free_var=true, only_equ_con=true))\n  sort!(problems)\n  @printf(\"%-7s  %15s  %15s  %15s\\n\",\n          \"Problem\", \"f(x)\", \"‖∇ℓ(x,λ)‖\", \"‖c(x)‖\")\n  for p in problems\n    nlp = CUTEstModel(p)\n    try\n      x, fx, nlx, ncx = solver(nlp)\n      @printf(\"%-7s  %15.8e  %15.8e  %15.8e\\n\", p, fx, nlx, ncx)\n    catch\n      @printf(\"%-7s  %s\\n\", p, \"failure\")\n    finally\n      finalize(nlp)\n    end\n  end\nend\n"}]}]}]},{type:a,value:c},{type:b,tag:e,props:{},children:[{type:a,value:"I'm gonna print the output of this one, so you can compare it with yours."}]},{type:a,value:c},{type:b,tag:g,props:{className:[h]},children:[{type:b,tag:i,props:{className:[j,k]},children:[{type:b,tag:d,props:{},children:[{type:a,value:"Problem             f(x)        ‖∇ℓ(x,λ)‖           ‖c(x)‖\nHS26      5.15931251e-07   9.88009545e-05   5.24359322e-05\nHS27      4.00000164e-02   5.13264248e-05   2.26312672e-09\nHS28      7.00144545e-09   9.46563681e-05   2.44249065e-15\nHS39     -1.00000010e+00   1.99856691e-08   1.61607518e-07\nHS40     -2.50011760e-01   4.52797064e-05   2.53246505e-05\nHS42      1.38577292e+01   5.06661945e-05   5.33092868e-05\nHS46      3.56533430e-06   9.98827045e-05   8.00086215e-05\nHS47      3.53637757e-07   9.71339790e-05   7.70496596e-05\nHS48      4.65110036e-10   4.85457139e-05   2.27798719e-15\nHS49      3.14248189e-06   9.94899395e-05   2.27488138e-13\nHS50      1.36244906e-12   2.16913725e-06   2.90632554e-14\nHS51      1.58249170e-09   8.52213221e-05   6.52675179e-15\nHS52      5.32664756e+00   3.35626559e-05   3.21155766e-14\nHS56     -3.45604528e+00   9.91076239e-05   3.14471179e-05\nHS6       5.93063756e-13   6.88804464e-07   9.61311292e-06\nHS61     -1.43646176e+02   1.06116455e-05   1.80421875e-05\nHS7      -1.73205088e+00   1.23808109e-11   2.60442422e-07\nHS77      2.41501014e-01   8.31210333e-05   7.75367223e-05\nHS78     -2.91972281e+00   2.27102179e-05   2.88776440e-05\nHS79      7.87776482e-02   4.77319205e-05   7.55827729e-05\nHS8      -1.00000000e+00   0.00000000e+00   2.39989802e-06\nHS9      -5.00000000e-01   1.23438228e-06   3.55271368e-15\n"}]}]}]},{type:a,value:c},{type:b,tag:e,props:{},children:[{type:a,value:"If you compare against the Hock-Schitkowski paper, you'll see that\nthe method converged for all 22 problems.\nConsidering our simplifications, this is a very exciting."}]},{type:a,value:c},{type:b,tag:e,props:{},children:[{type:a,value:"That's all for now. Use our RSS feed to keep updated."}]}]},dir:"\u002Fblog",path:"\u002Fblog\u002F2017-02-17-nlpmodelsjl-and-cutestjl-constrained-optimization",extension:".md",createdAt:L,updatedAt:L,bodyPlainText:"\n# NLPModels.jl and CUTEst.jl&#58; Constrained optimization\n\nThis is a continuation of [this\npost](https:\u002F\u002Fabelsiqueira.github.io{{local_prefix}}nlpmodelsjl-cutestjl-and-other-nonlinear-optimization-packages-on-julia\u002F).\nAnd again, you can follow the commands of this post in the\n[asciinema](https:\u002F\u002Fasciinema.org\u002Fa\u002F103654).\n\nIf you followed along last post, you should know the basics of our\nNLPModels API, including CUTEst access.\n\nOne thing I didn't explore, though, was constrained problems.\nIt'd complicate too much.\n\nHowever, now that we know how to handle the basics, we can move to the\nadvanced.\n\n**Nonlinear Programming format**\n\nThe NLPModels internal structure is based on the CUTEst way of storing a\nproblem.\nWe use the following form for the optimization problem:\n\n$$\n\\begin{align}\n\\min \\quad & f(x) \\\\\ns.t. \\quad & c_L \\leq c(x) \\leq c_U \\\\\n& \\ell \\leq x \\leq u\\end{align}\n$$\n\nGiven an `AbstractNLPModel` named `nlp`, the values for $\\ell$, $u$, $c_L$ and\n$c_U$ are stored in an `NLPModelMeta` structure, and can be accessed by\nthrough `nlp.meta`.\n\nLet's look back at the simple Rosenbrock problem of before.\n\n```\nusing NLPModels\n\nf(x) = (x[1] - 1)^2 + 100*(x[2] - x[1]^2)^2\nx0 = [-1.2; 1.0]\nnlp = ADNLPModel(f, x0)\nprint(nlp.meta)\n```\n\nYou should be seeing this:\n\n```\nMinimization problem Generic\nnvar = 2, ncon = 0 (0 linear)\nlvar = -Inf  -Inf\nuvar = Inf  Inf\nlcon = ∅\nucon = ∅\nx0 = -1.2  1.0\ny0 = ∅\nnnzh = 4\nnnzj = 0\n```\n\nAlthough the meaning of these values is reasonably straigthforward, I'll explain a bit.\n\n- `nvar` is the number of variables in a problem;\n- `ncon` is the number of constraints, without counting the bounds;\n- `lvar` is the vector $\\ell$, the lower bounds on the variables;\n- `uvar` is the vector $u$, the upper bounds on the variables;\n- `lcon` is the vector $c_L$, the lower bounds of the constraints function;\n- `ucon` is the vector $c_U$, the upper bounds of the constraints function;\n- `x0` is the initial approximation to the solution, aka the starting point;\n- `y0` is the initial approximation to the Lagrange multipliers;\n- `nnzh` is the number of nonzeros on the Hessian¹;\n- `nnzj` is the number of nonzeros on the Jacobian¹;\n\n_¹ `nnzh` and `nnzj` are not consistent between models, because some consider the dense matrix, and for the Hessian, some consider only the triangle. However, if you're possibly considering using `nnzh`, you're probably looking for `hess_coord` too, and `hess_coord` returns with the correct size._\n\nThese values can be accessed directly as fields in `meta` with the same name above.\n\n```\nnlp.meta.ncon\nnlp.meta.x0\nnlp.meta.lvar\n```\n\n**Bounds**\n\nNow, let's create a bounded problem.\n\n```\nnlp = ADNLPModel(f, x0, lvar=zeros(2), uvar=[0.4; 0.6])\nprint(nlp.meta)\n```\n\nNow the bounds are set, and you can access them with\n\n```\nnlp.meta.lvar\nnlp.meta.uvar\n```\n\nThat's pretty much it. For `SimpleNLPModel`, it's the same thing.\n`MathProgNLPModel` inherits the bounds, as expected:\n\n```\nusing JuMP\n\njmp = Model()\nu = [0.4; 0.6]\n@variable(jmp, 0 \u003C= x[i=1:2] \u003C= u[i], start=(x0[i]))\n@NLobjective(jmp, Min, (x[1] - 1)^2 + 100*(x[2] - x[1]^2)^2)\nmpbnlp = MathProgNLPModel(jmp)\nprint(mpbnlp.meta)\n```\n\nFor CUTEst, there is no differentiation on creating a problem with bounds or\nnot, since it uses the internal description of the problem.\nFor instance, `HS4` is a bounded problem.\n\n```\nusing CUTEst\n\nclp = CUTEstModel(\"HS4\")\nprint(clp.meta)\nfinalize(clp)\n```\n\nNotice that it can happen that one or more of the variables is unlimited\n(lower, upper or both). This is represented by the value `Inf` in Julia.\nThis should be expected since the unconstrained problem already used these\n`Inf` values.\n\nOn the other hand, it could happen that $\\ell_i = u_i$, in which case the\nvariable is fixed, or that $\\ell_i \u003E u_i$, in which case the variable (and the\nproblem) is infeasible.\nNote that `NLPModels` only creates the model, it doesn't check whether it is\nfeasible or not, even in this simple example. That said, CUTEst shouldn't have\nany infeasible variable.\n\nFurthermore, all these types of bounds can be accessed from `meta`. Notice that\nthere are 6 possible situations:\n\n- Free variables, stored in `meta.ifree`;\n- Fixed variables, stored in `meta.ifix`;\n- Variables bounded below, stored in `meta.ilow`;\n- Variables bounded above, stored in `meta.iupp`;\n- Variables bounded above and below, stored in `meta.irng`;\n- Infeasible variables, stored in `meta.iinf`.\n\nHere is one example with one of each of them\n\n```\nnlp = ADNLPModel(x-\u003Edot(x,x), zeros(6),\n  lvar = [-Inf, -Inf, 0.0, 0.0, 0.0,  0.0],\n  uvar = [ Inf,  1.0, Inf, 1.0, 0.0, -1.0])\nnlp.meta.ifree\nnlp.meta.ifix\nnlp.meta.ilow\nnlp.meta.iupp\nnlp.meta.irng\nnlp.meta.iinf\n```\n\n**Constraints**\n\nConstraints are stored in NLPModels following in the format $c_L \\leq c(x) \\leq c_U$.\nThat means that an equality constraint happens when $c_{L_j} = c_{U_j}$.\nLet's look at how to create a problem with constraints.\n\nFor `ADNLPModel`, you need to pass three keywords arguments: `c`, `lcon` and `ucon`,\nwhich represent $c(x)$, $c_L$ and $c_U$, respectively.\nFor instance, the problem\n\n$$\n\\begin{align}\n\\min \\quad & x_1^2 + x_2^2 \\\\\ns.t. \\quad & x_1 + x_2 = 1\n\\end{align}\n$$\n\nis created by doing\n\n```\nc(x) = [x[1] + x[2] - 1]\nlcon = [0.0]\nucon = [0.0]\nnlp = ADNLPModel(x-\u003Edot(x,x), zeros(2), c=c, lcon=lcon, ucon=ucon)\n```\n\nor alternatively, if you don't want the intermediary functions\n\n```\nnlp = ADNLPModel(x-\u003Edot(x,x), zeros(2), c=x-\u003E[x[1]+x[2]-1], lcon=[0.0], ucon=[0.0])\n```\n\nAnother possibility is to do\n\n```\nnlp = ADNLPModel(x-\u003Edot(x,x), zeros(2), c=x-\u003E[x[1]+x[2]], lcon=[1.0], ucon=[1.0])\n```\n\nPersonally, I prefer the former.\n\nFor inequalities, you can have only lower, only upper, and both.\nThe commands\n\n```\nnlp = ADNLPModel(x-\u003Edot(x,x), zeros(2),\n  c=x-\u003E[x[1] + x[2]; 3x[1] + 2x[2]; x[1]*x[2]],\n  lcon = [-1.0; -Inf; 1.0],\n  ucon = [Inf;   3.0; 2.0])\n```\n\nimplement the problem\n\n$$\n\\begin{align}\n\\min \\quad & x_1^2 + x_2^2 \\\\\ns.t. \\quad & x_1 + x_2 \\geq -1 \\\\\n& 3x_1 + 2x_2 \\leq 3 \\\\\n& 1 \\leq x_1x_2 \\leq 2.\n\\end{align}\n$$\n\nAgain, the types of constraints can be accessed in `meta`, through\n`nlp.meta.jfix`, `jfree`, `jinf`, `jlow`, `jrng` and `jupp`.\nNotice if you forget to set `lcon` and `ucon`, there will be no\nconstraints, even though `c` is set. This is because the number of\nconstraints is taken from the lenght of these vectors.\n\nNow, to access these constraints, let's consider this simple problem.\n\n```\nnlp = ADNLPModel(f, x0, c=x-\u003E[x[1]*x[2] - 0.5], lcon=[0.0], ucon=[0.0])\n```\n\nThe function `cons` return $c(x)$.\n\n```\ncons(nlp, nlp.meta.x0)\n```\n\nThe function `jac` returns the Jacobian of $c$. `jprod` and `jtprod` the\nJacobian product times a vector, and `jac_op` the LinearOperator.\n\n```\njac(nlp, nlp.meta.x0)\njprod(nlp, nlp.meta.x0, ones(2))\njtprod(nlp, nlp.meta.x0, ones(1))\nJ = jac_op(nlp, nlp.meta.x0)\nJ * ones(2)\nJ' * ones(1)\n```\n\nTo get the Hessian we'll use the same functions as the unconstrained case,\nwith the addition of a keyword parameter `y`.\n\n```\ny = [1e4]\nhess(nlp, nlp.meta.x0, y=y)\nhprod(nlp, nlp.meat.x0, ones(2))\nH = hess_op(nlp, nlp.meta.x0, y=y)\nH * ones(2)\n```\n\nIf you want to ignore the objective function, or scale it by some value,\nyou can use the keyword parameter `obj_weight`.\n\n```\ns = 0.0\nhess(nlp, nlp.meta.x0, y=y, obj_weight=s)\nhprod(nlp, nlp.meat.x0, ones(2), obj_weight=s)\nH = hess_op(nlp, nlp.meta.x0, y=y, obj_weight=s)\nH * ones(2)\n```\n\nCheck the\n[API](http:\u002F\u002Fjuliasmoothoptimizers.github.io\u002FNLPModels.jl\u002Fstable\u002Fapi.html)\nfor more details.\n\nWe can also create a constrained JuMP model.\n\n```\nx0 = [-1.2; 1.0]\njmp = Model()\n@variable(jmp, x[i=1:2], start=(x0[i]))\n@NLobjective(jmp, Min, (x[1] - 1)^2 + 100*(x[2] - x[1]^2)^2)\n@NLcontraint(jmp, x[1]*x[2] == 0.5)\nmpbnlp = MathProgNLPModel(jmp)\ncons(mpbnlp, mpbnlp.meta.x0)\njac(mpbnlp, mpbnlp.meta.x0)\nhess(mpbnlp, mpbnlp.meta.x0, y=y)\n```\n\nAnd again, the access in CUTEst problems is the same.\n\n```\nclp = CUTEstModel(\"BT1\")\ncons(clp, clp.meta.x0)\njac(clp, clp.meta.x0)\nhess(clp, clp.meta.x0, y=clp.meta.y0)\nfinalize(clp)\n```\n\n**Convenience functions**\n\nThere are some convenience functions to check whether a problem has only\nequalities, only bounds, etc.\nFor clarification, we're gonna say function constraint to refer to constraints that are not bounds.\n\n- `has_bounds`: Returns `true` is variable has bounds.\n- `bound_constrained`: Returns `true` if `has_bounds` and no function\n  constraints;\n- `unconstrained`: No function constraints nor bounds;\n- `linearly_constrained`: There are function constraints, and they are\n  linear; _obs: even though a `bound_constrained` problem is linearly\n  constrained, this will return false_.\n- `equality_constrained`: There are function constraints, and they are all equalities;\n- `inequality_constrained`: There are function constraints, and they are all inequalities;\n\n**Example solver**\n\nLet's implement a \"simple\" solver for constrained optimization.\nOur solver will loosely follow the Byrd-Omojokun implementation of\n\n\u003E M. Lalee, J. Nocedal, and T. Plantenga. **On the implementation of an algorithm for large-scale equality constrained optimization**. SIAM J. Optim., Vol. 8, No. 3, pp. 682-706, 1998.\n\n```\nfunction solver(nlp :: AbstractNLPModel)\n  if !equality_constrained(nlp)\n    error(\"This solver is for equality constrained problems\")\n  elseif has_bounds(nlp)\n    error(\"Can't handle bounds\")\n  end\n\n  x = nlp.meta.x0\n\n  fx = obj(nlp, x)\n  cx = cons(nlp, x)\n\n  ∇fx = grad(nlp, x)\n  Jx = jac_op(nlp, x)\n\n  λ = cgls(Jx', -∇fx)[1]\n  ∇ℓx = ∇fx + Jx'*λ\n  norm∇ℓx = norm(∇ℓx)\n\n  Δ = max(0.1, min(100.0, 10norm∇ℓx))\n  μ = 1\n  v = zeros(nlp.meta.nvar)\n\n  iter = 0\n  while (norm∇ℓx \u003E 1e-4 || norm(cx) \u003E 1e-4) && (iter \u003C 10000)\n    # Vertical step\n    if norm(cx) \u003E 1e-4\n      v = cg(Jx'*Jx, -Jx'*cx, radius=0.8Δ)[1]\n      Δp = sqrt(Δ^2 - dot(v,v))\n    else\n      fill!(v, 0)\n      Δp = Δ\n    end\n\n    # Horizontal step\n    # Simplified to consider only ∇ℓx = proj(∇f, Nu(A))\n    B = hess_op(nlp, x, y=λ)\n    B∇ℓx = B * ∇ℓx\n    gtBg = dot(∇ℓx, B∇ℓx)\n    gtγ = dot(∇ℓx, ∇fx + B * v)\n    t = if gtBg \u003C= 0\n      norm∇ℓx \u003E 0 ? Δp\u002Fnorm∇ℓx : 0.0\n    else\n      t = min(gtγ\u002FgtBg, Δp\u002Fnorm∇ℓx)\n    end\n\n    d = v - t * ∇ℓx\n\n    # Trial step acceptance\n    xt = x + d\n    ft = obj(nlp, xt)\n    ct = cons(nlp, xt)\n    γ = dot(d, ∇fx) + 0.5*dot(d, B * d)\n    θ = norm(cx) - norm(Jx * d + cx)\n    normλ = norm(λ, Inf)\n    if θ \u003C= 0\n      μ = normλ\n    elseif normλ \u003E γ\u002Fθ\n      μ = min(normλ, 0.1 + γ\u002Fθ)\n    else\n      μ = 0.1 + γ\u002Fθ\n    end\n    Pred = -γ + μ * θ\n    Ared = fx - ft + μ * (norm(cx) - norm(ct))\n\n    ρ = Ared\u002FPred\n    if ρ \u003E 1e-2\n      x .= xt\n      fx = ft\n      cx .= ct\n      ∇fx = grad(nlp, x)\n      Jx = jac_op(nlp, x)\n      λ = cgls(Jx', -∇fx)[1]\n      ∇ℓx = ∇fx + Jx'*λ\n      norm∇ℓx = norm(∇ℓx)\n      if ρ \u003E 0.75 && norm(d) \u003E 0.99Δ\n        Δ *= 2.0\n      end\n    else\n      Δ *= 0.5\n    end\n\n    iter += 1\n  end\n\n  return x, fx, norm∇ℓx, norm(cx)\nend\n```\n\nToo loosely, in fact.\n\n- The horizontal step computes only the Cauchy step;\n- No special updates;\n- No second-order correction;\n- No efficient implementation beyond the easy-to-do.\n\nTo test how good it is, let's run on the Hock-Schittkowski constrained problems.\n\n```\nfunction runcutest()\n  problems = filter(x-\u003Econtains(x, \"HS\") && length(x) \u003C= 5, CUTEst.select(only_free_var=true, only_equ_con=true))\n  sort!(problems)\n  @printf(\"%-7s  %15s  %15s  %15s\\n\",\n          \"Problem\", \"f(x)\", \"‖∇ℓ(x,λ)‖\", \"‖c(x)‖\")\n  for p in problems\n    nlp = CUTEstModel(p)\n    try\n      x, fx, nlx, ncx = solver(nlp)\n      @printf(\"%-7s  %15.8e  %15.8e  %15.8e\\n\", p, fx, nlx, ncx)\n    catch\n      @printf(\"%-7s  %s\\n\", p, \"failure\")\n    finally\n      finalize(nlp)\n    end\n  end\nend\n```\n\nI'm gonna print the output of this one, so you can compare it with yours.\n\n```\nProblem             f(x)        ‖∇ℓ(x,λ)‖           ‖c(x)‖\nHS26      5.15931251e-07   9.88009545e-05   5.24359322e-05\nHS27      4.00000164e-02   5.13264248e-05   2.26312672e-09\nHS28      7.00144545e-09   9.46563681e-05   2.44249065e-15\nHS39     -1.00000010e+00   1.99856691e-08   1.61607518e-07\nHS40     -2.50011760e-01   4.52797064e-05   2.53246505e-05\nHS42      1.38577292e+01   5.06661945e-05   5.33092868e-05\nHS46      3.56533430e-06   9.98827045e-05   8.00086215e-05\nHS47      3.53637757e-07   9.71339790e-05   7.70496596e-05\nHS48      4.65110036e-10   4.85457139e-05   2.27798719e-15\nHS49      3.14248189e-06   9.94899395e-05   2.27488138e-13\nHS50      1.36244906e-12   2.16913725e-06   2.90632554e-14\nHS51      1.58249170e-09   8.52213221e-05   6.52675179e-15\nHS52      5.32664756e+00   3.35626559e-05   3.21155766e-14\nHS56     -3.45604528e+00   9.91076239e-05   3.14471179e-05\nHS6       5.93063756e-13   6.88804464e-07   9.61311292e-06\nHS61     -1.43646176e+02   1.06116455e-05   1.80421875e-05\nHS7      -1.73205088e+00   1.23808109e-11   2.60442422e-07\nHS77      2.41501014e-01   8.31210333e-05   7.75367223e-05\nHS78     -2.91972281e+00   2.27102179e-05   2.88776440e-05\nHS79      7.87776482e-02   4.77319205e-05   7.55827729e-05\nHS8      -1.00000000e+00   0.00000000e+00   2.39989802e-06\nHS9      -5.00000000e-01   1.23438228e-06   3.55271368e-15\n```\n\nIf you compare against the Hock-Schitkowski paper, you'll see that\nthe method converged for all 22 problems.\nConsidering our simplifications, this is a very exciting.\n\nThat's all for now. Use our RSS feed to keep updated.\n"},prev:{slug:"2017-02-20-minicurso-de-julia-no-ix-simposio-de-analise-numerica-e-otimizacao-da-ufpr",title:"Minicurso de Julia no IX Simpósio de Análise Numérica e Otimização da UFPR"},next:{slug:"2017-02-07-nlpmodelsjl-cutestjl-and-other-nonlinear-optimization-packages-on-julia",title:"NLPModels.jl, CUTEst.jl and other Nonlinear Optimization Packages on Julia"}}],fetch:{},mutations:void 0}}("text","element","\n","code","p","li","div","nuxt-content-highlight","pre","language-text","line-numbers",".","strong"," and ",";",", ","a","ul","true","nofollow","noopener","noreferrer","_blank","lcon","ucon","nnzh","meta","NLPModels.jl and CUTEst.jl: Constrained optimization","nnzj","em","hess_coord","Inf","c","The function ","has_bounds",": Returns ","bound_constrained","2022-12-23T23:04:30.335Z")));