<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/">
    <channel>
        <title>Abel Soares Siqueira</title>
        <link>https://abelsiqueira.github.io/tag/julia/feed.xml</link>
        <description>RSS feed for abelsiqueira.github.io</description>
        <lastBuildDate>Fri, 23 Dec 2022 14:24:34 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/nuxt-community/feed-module</generator>
        <category>Nuxt.js</category>
        <item>
            <title><![CDATA[Can Python with Julia be faster than low-level code?]]></title>
            <link>https://abelsiqueira.github.io/blog/2022-02-11-python-and-julia-3</link>
            <guid>https://abelsiqueira.github.io/blog/2022-02-11-python-and-julia-3</guid>
            <pubDate>Fri, 11 Feb 2022 00:00:00 GMT</pubDate>
            <content:encoded><![CDATA[
# Can Python with Julia be faster than low-level code?

I worked on this blog post working for eScience Center.
Co-authored by Faruk Diblen.

Check the blog post [here](https://blog.esciencecenter.nl/can-python-with-julia-be-faster-than-low-level-code-cd71a72fbcf4).
]]></content:encoded>
            <enclosure url="https://miro.medium.com/max/720/1*Bta22zk1Dv3nsfacxEyK0A.jpeg" length="0" type="image/jpeg"/>
        </item>
        <item>
            <title><![CDATA[Speed up your Python code using Julia]]></title>
            <link>https://abelsiqueira.github.io/blog/2022-01-26-python-and-julia-2</link>
            <guid>https://abelsiqueira.github.io/blog/2022-01-26-python-and-julia-2</guid>
            <pubDate>Wed, 26 Jan 2022 00:00:00 GMT</pubDate>
            <content:encoded><![CDATA[
# Speed up your Python code using Julia

I worked on this blog post working for eScience Center.
Co-authored by Faruk Diblen.

Check the blog post [here](https://blog.esciencecenter.nl/speed-up-your-python-code-using-julia-f97a6c155630).
]]></content:encoded>
            <enclosure url="https://miro.medium.com/max/720/1*4pbcALDvrchxB1s72pD-mA.jpeg" length="0" type="image/jpeg"/>
        </item>
        <item>
            <title><![CDATA[How to call Julia code from Python]]></title>
            <link>https://abelsiqueira.github.io/blog/2022-01-19-python-and-julia-1</link>
            <guid>https://abelsiqueira.github.io/blog/2022-01-19-python-and-julia-1</guid>
            <pubDate>Wed, 19 Jan 2022 00:00:00 GMT</pubDate>
            <content:encoded><![CDATA[
# How to call Julia code from Python

I worked on this blog post working for eScience Center.
Co-authored by Faruk Diblen.

Check the blog post [here](https://blog.esciencecenter.nl/how-to-call-julia-code-from-python-8589a56a98f2).
]]></content:encoded>
            <enclosure url="https://miro.medium.com/max/720/1*rO_VWgfXj4zuFDtX28Uc1g.jpeg" length="0" type="image/jpeg"/>
        </item>
        <item>
            <title><![CDATA[Migração do site para Franklin.jl]]></title>
            <link>https://abelsiqueira.github.io/blog/2021-06-28-migracao-franklin</link>
            <guid>https://abelsiqueira.github.io/blog/2021-06-28-migracao-franklin</guid>
            <pubDate>Mon, 28 Jun 2021 00:00:00 GMT</pubDate>
            <content:encoded><![CDATA[
# Migração do site para Franklin.jl

Há algum tempo o Jekyll tem me dado trabalho.
Como não sou usuário de Ruby, volta e meia falta alguma coisa na minha instalação e eu passo horas tentando corrigir o problema pra fazer meu site rodar localmente.
Para o CiDAMO, eu considerei o Hugo, já que tem mais suporte e eu não seria a única pessoa a cuidar do site.
Já para meu site pessoal, e o site do [Julia Smooth Optimizers](https://juliasmoothoptimizers.github.io), resolvi usar Franklin, já que eu quero mais é usar Julia mesmo.

## Vantagens

**Franklin.jl é Julia,** então eu tenho mais facilidade em manter e até mesmo de contribuir.
O site é um repositório Julia, então tudo está dentro do esperado.

**Franklin.jl roda Julia,** isto é, dá pra usar Julia pra gerar conteúdo **E** pra fazer tutoriais.
Por exemplo, a lista de posts na página principal do blog é gerada por um script em Julia que lê os diretórios e imprime o markdown para a página.
Por outro lado, os tutoriais do JSO e minhas notas de Cálculo Numérico rodam o código mostrado e a saída é mostrada, então o código está sempre atualizado (ou visivelmente quebrado).

**_Builda_ no GitHub actions.** Então, eu consigo hostear no próprio github. É como GHA é o padrão do Julia, é fácil de achar suporte.

## Desvantagens

**Pode ficar lento,** já que tem que compilar e rodar o código.
Por exemplo, as notas de cálculo numérico levam 40 minutos para rodar, porque tem dezenas de imagens e algumas animações.
Eu tive que fazer uma gambiarra para separar as notas do site. Falo mais sobre isso [aqui](#gambiarra).

**Não é óbvio como manter layouts diferentes.** Por exemplo, se eu quiser ter um layout de `blog` separado de um layout de `page`, não parece óbvio como.
Eu posso colocar `if`s pra separar conteúdo, mas não é tão natural.

**Ainda não está muito estável.** Naturalmente, já que é bem novo, mas dá pra sentir quando se precisa achar alguma coisa específica.

## Gambiarra

Meu site tem 3 partes:

- a base, que consiste principalmente de material estático que eu pretendo atualizar de vez em quando (novos pacotes, pesquisa, orientações, cargos, etc.);
- as notas de cálculo numérico, que depois de pronto deve ver pouca atualização, mas que leva 40 minutos pra compilar no GitHub Actions.
- o blog, que idealmente vê atualizações frequentes, e por isso vou fingir que terá atualizações frequentes.

Se eu coloco todos juntos, então qualquer mudança na base ou no blog leva 40 minutos pra compilar no GitHub Actions por causa das notas de cálculo numérico.

Por outro lado, se eu separo todas, mudanças no CSS, layout, javascript, etc., não são atualizadas.

A solução, simples, é de só manter os arquivos de CSS, layout, javascript, etc., na base e copiá-los no build do GitHub Actions para os outros repositórios.
Além disso, adiciono essas pastas no `.gitignore`, e localmente é só copiá-las para os repositórios que precisam dela.
Aqui a adição ao GitHub actions:

```yaml
- name: Clone abelsiqueira.github.io
  run: |
    wget https://github.com/abelsiqueira/abelsiqueira.github.io/archive/refs/heads/main.zip
    unzip main.zip
    mv abelsiqueira.github.io-main/_layout .
    mv abelsiqueira.github.io-main/_css .
    mv abelsiqueira.github.io-main/_libs .
    rm -rf abelsiqueira.github.io-main main.zip
```

## Conclusão

No fim das contas, sair do Jekyll para o Franklin vai ser uma experiência interessante.
Devo fazer alguns posts atualizados usando as novas capacidades do Franklin, e aí veremos se valeu a pena ou não.
Também queria ter usado a oportunidade para aprender o Hugo, mas uma coisa de cada vez.
]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Koch snowflakes for the holidays]]></title>
            <link>https://abelsiqueira.github.io/blog/2018-12-12-koch-snowflakes-for-the-holidays</link>
            <guid>https://abelsiqueira.github.io/blog/2018-12-12-koch-snowflakes-for-the-holidays</guid>
            <pubDate>Wed, 12 Dec 2018 00:00:00 GMT</pubDate>
            <content:encoded><![CDATA[
# Koch snowflakes for the holidays

[Code for these images](https://github.com/abelsiqueira/koch-holidays).

I hope you're familiar with the [Koch curve
fractal or snowflake](https://en.wikipedia.org/wiki/Koch_snowflake).
It essentially consists taking a line segment, splitting it in three, and substituting
the middle part by two segments that form an equilateral triangle without the base.
From one segment you obtain four. For each new segment, repeat the process.

Images:

![](/blog/koch/line-koch-0.png)

![](/blog/koch/line-koch-1.png)
![](/blog/koch/line-koch-2.png)
![](/blog/koch/line-koch-3.png)
![](/blog/koch/line-koch-4.png)
![](/blog/koch/line-koch-5.png)
![](/blog/koch/line-koch-6.png)
![](/blog/koch/line-koch-7.png)

The most important aspect of the koch line is that it looks awesome. Furthermore, you
can do it for any image that is a collection of segments. In particular, regular
polygons, both outward and inward.

![](/blog/koch/polygon-2.png)
![](/blog/koch/polygon-reverse-2.png)
![](/blog/koch/polygon-3.png)
![](/blog/koch/polygon-reverse-3.png)
![](/blog/koch/polygon-4.png)
![](/blog/koch/polygon-reverse-4.png)
![](/blog/koch/polygon-5.png)
![](/blog/koch/polygon-reverse-5.png)
![](/blog/koch/polygon-6.png)
![](/blog/koch/polygon-reverse-6.png)
![](/blog/koch/polygon-7.png)
![](/blog/koch/polygon-reverse-7.png)
![](/blog/koch/polygon-8.png)
![](/blog/koch/polygon-reverse-8.png)

Another way to define the four new segments is this: Given the endpoints of the segment
$P_1$ and $P_2$, we define $\vec{v} = \vec{P_1P_2}$. The five points that define the
four new segments, in order, are $P_1$, $P_1 + \frac{1}{3}\vec{v}$,
$P_1 + \frac{1}{2}\vec{v} + \alpha R\vec{v}$, $P1 + \frac{2}{3}\vec{v}$ and $P_2$,
where $\alpha = \sqrt{3}/6$.
A simple thing one can do is change the value of $\alpha$ to obtain different images:

![](/blog/koch/star.png)
Using a diamond with $\alpha = 1.25$.

![](/blog/koch/reverse-star.png)
Using a square with $\alpha = 1.2$.

![](/blog/koch/stargon-3.png)
![](/blog/koch/stargon-4.png)
![](/blog/koch/stargon-5.png)
Using polygons with $N$ equals to 3, 4 and 5 sides, and $\alpha = 3\sqrt{N}/5$.

![](/blog/koch/tri-3.png)
![](/blog/koch/tri-4.png)
![](/blog/koch/tri-5.png)
Using a line segment from the center to the vertex and back and to next vertex, etc.,
with $\alpha = 1$.

And to close off, here's a traditional triangle, repeated, with alternating colors.
![](/blog/koch/koch.png)

[Here a larger scale version](/blog/koch/koch-large.png)
![](/blog/koch/koch-large.png)

Happy holidays!
![](/blog/koch/koch-julia.png)
]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Package Development in Julia 1.0 using the REPL]]></title>
            <link>https://abelsiqueira.github.io/blog/2018-12-05-package-development-on-julia-10</link>
            <guid>https://abelsiqueira.github.io/blog/2018-12-05-package-development-on-julia-10</guid>
            <pubDate>Wed, 05 Dec 2018 00:00:00 GMT</pubDate>
            <content:encoded><![CDATA[
# Package Development in Julia 1.0 using the REPL

This is a quick post on package development in Julia 1.0. Let me know if you're
interested in more, and what.

Suppose you're developing a package - say, `MyPackage.jl` - whether from scratch, or
updating from Julia 0.6.
First, and foremost, you need to be able to run and test it.
On Julia 1.0, this is one possible way, while using the REPL.

**Choose a folder to develop it and create the necessary files**

You'll need

```
- MyPackage.jl/
  - src/
    - MyPackage.jl
  - test/
    - runtests.jl
  - README.md (eventually)
  - LICENSE.md (eventually)
  - .travis.yml (eventually)
```

The file `src/MyPackage.jl` is the file included by Julia when you enter
`Using MyPackage`. `test/runtests.jl` is what is run to test your package. It is
required if you intend to publish your package, but most important, you need to test
your package before trying to publish it. README gives information about your package,
LICENSE gives information about its license, and .travis.yml defines the online testing
with continuous integration by Travis. These are usual, but not require when you're
starting to develop your package.

For instance, consider the following files

```
# src/MyPackage.jl
module MyPackage

export pi_approximation

function pi_approximation()
  return 22.0 / 7.0
end

end # module
```

```
# test/runtests.jl
using MyPackage, Test

function tests()
  @testset "Subset of tests" begin
    @test pi_approximation() ≈ pi atol=1e-2
  end
end

tests()
```

**Open the REPL and add your package under the development version**

_I assume Linux, but this will work with minor modifications on OSX and Windows._

On the terminal, enter julia to open the REPL

```
julia>
```

Enter `pkg` mode by pressing `]`.

```
(v1.0) pkg>
```

Inform `pkg` that your package folder exists and is under development with `dev`.

```
(v1.0) pkg> dev SOME_PATH/MyPackage.jl

[ Info: Assigning UUID XXXXXX to MyPackage
 Resolving package versions...
  Updating `~/.julia/environments/v1.0/Project.toml`
  [XXX] + MyPackage v0.0.0 [`SOME_PATH/MyPackage.jl`]
  Updating `~/.julia/environments/v1.0/Manifest.toml`
  [XXX] + MyPackage v0.0.0 [`SOME_PATH/MyPackage.jl`]
```

Check that you can see your package

```
(v1.0) pkg> status
    Status `~/.julia/environments/v1.0/Project.toml`
  ...
  [XXX] MyPackage v0.0.0 [`SOME_PATH/MyPackage.jl`]
  ...
```

Check that everything passes according to your (evil) plan.

```
(v1.0) pkg> test MyPackage
   Testing MyPackage
 Resolving package versions...
    Status `/tmp/tmpO7CsSr/Manifest.toml`
  [XXX] MyPackage v0.0.0 [`SOME_PATH/MyPackage.jl`]
  ...
Test Summary:   | Pass  Total
Subset of tests |    1      1
   Testing MyPackage tests passed
```

When you're done with your package, you can `rm MyPackage` to remove your package from
consideration without deleting the code.

If your package was already released, then it'll have a different version number. Other
differences may apply.
]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Optimizing the Dollar Game from Numberphile]]></title>
            <link>https://abelsiqueira.github.io/blog/2018-09-04-the-dollar-game-from-numberphile</link>
            <guid>https://abelsiqueira.github.io/blog/2018-09-04-the-dollar-game-from-numberphile</guid>
            <pubDate>Tue, 04 Sep 2018 00:00:00 GMT</pubDate>
            <content:encoded><![CDATA[
# Optimizing the Dollar Game from Numberphile

I just watched [The Dollar Game -
Numberphile](https://www.youtube.com/watch?v=U33dsEcKgeQ), in which a game involving graphs is presented.
I recommend you watch the video for complete information.

The game involves a graph with integer values on its nodes, positive and
negative. For instance, the following graph:

![](/blog/2018-09-04/example.png)

Each **node** corresponds to a person, the **node value** is the amount of
money that person has, the **edges** are the people that person can give or
take money from.
The objective of the game is to have everyone have a non-negative amount of money.
In each **move** of the game, one person decides to give or take money,
**however**, that person either takes 1 dollar from each of their connections,
or gives 1 dollar to each one.

On Numberphile, there are two questions: (i) does the problem has a solution?
(ii) what's the least amount of moves to find it?
I've decided to implement this problem using optimization, since it looked
almost straightfoward. The optimization model was indeed very simple, and it
took more time drawing graphs than modelling. Still fun though.

## The model

Given the undirected graph $G = (V, E)$, and values $w_i, i \in V$, our
model is based on the following observations:

- The order of moves is irrelevant;
- Whether the move is a _give_ or a _take_, is just a question of **sign**;
- The value of a node after the moves can be computed by accounting for the
  moves done by the done and by its neighbours.

Hence, we can model it using two non-negative integer variables $g_i$ and
$t_i$ storing the number of gives and takes of node $i$. Notice that we
could use $y_i = g_i - t_i$, but this is more descriptive.

- Objective: minimize the number of moves

$$\min \sum_i (g_i + t_i)$$

- Constraint: after the moves, the values of the nodes should be non-negative

$$
w_i + \sum_i (t_i - g_i) |N_i| + \sum_{j \in N_i} (g_j - t_j) \geq 0,
\qquad \forall i \in V.
$$

As it turns out, it's a very simple model. The implementation is also very
simple. We're using [Julia Language](https://julialang.org) with the
[JuMP](https://juliaopt.org) modelling language, and the
[LightGraphs package from JuliaGraphs](https://juliagraphs.github.io/).
Here's the code:

```
function dollar_game(g, W)
   nv = length(vertices(g))
   model = Model(solver = CbcSolver())
   @variable(model, give[1:nv] >= 0, Int)
   @variable(model, take[1:nv] >= 0, Int)
   @objective(model, Min, sum(give[i] + take[i] for i = 1:nv))
   @expression(model, x[i=1:nv], W[i] +
               (take[i] - give[i]) * length(neighbors(g, i)) +
               sum(give[j] - take[j] for j = neighbors(g, i)))
   @constraint(model, x .>= 0)
   status = solve(model)
   println("status = $status")
   if status != :Optimal
      return zeros(nv), W
   end

   give = Int.(getvalue(give))
   take = Int.(getvalue(take))
   sol = Int.(getvalue(x))
   println("give = $give")
   println("take = $take")
   println("sol = $sol")
   return give - take, sol
end
```

The code should be pretty self-explanatory, but ping me on twitter if you need clarification.

Using the results and ~~_mad plotting skillz_~~ the packages Plots and GR, we
obtain a solution for the problem above. The moves are illustrated below, where
blue means giving, and red means taking.

![](/blog/2018-09-04/example-000.png)
![](/blog/2018-09-04/example-001.png)
![](/blog/2018-09-04/example-002.png)
![](/blog/2018-09-04/example-003.png)
![](/blog/2018-09-04/example-004.png)
![](/blog/2018-09-04/example-005.png)

The full code is available at [GitHub](https://github.com/abelsiqueira/DollarGame.jl).
]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[My experience in the JuMP-dev annual workshop]]></title>
            <link>https://abelsiqueira.github.io/blog/2018-07-04-my-experience-in-the-jump-dev-annual-workshop</link>
            <guid>https://abelsiqueira.github.io/blog/2018-07-04-my-experience-in-the-jump-dev-annual-workshop</guid>
            <pubDate>Wed, 04 Jul 2018 00:00:00 GMT</pubDate>
            <content:encoded><![CDATA[
# My experience in the JuMP-dev annual workshop

Last week I had the pleasure of being invited to the [Second annual JuMP-dev
workshop](http://www.juliaopt.org/meetings/bordeaux2018/), which happened in June 27-29,
2018 at Bordeaux, France.
I've presented the packages from the [Julia Smooth
Optimizers](https://JuliaSmoothOptimizers.github.io) organization, and had a very good
time meeting with the JuMP developers.

For those still unaware, [JuMP](https://github.com/JuliaOpt/JuMP.jl) is a modelling
language for Mathematical Programming written in Julia. It provides access to a few
different solvers for many kinds of problems, and it works inside of Julia, so one can
enjoy the advantages of having a robust language if there is a need for advanced usage.

I've used Julia in classes since 2016 for teaching numerical calculus, and the packages
of Julia Smooth Optimizers for nonlinear optimization this last semester.
I've taught a quick tutorial on JuMP in that class to solve a few nonlinear problems,
and discuss the starting point dependency of nonlinear solvers.
The notebook can be found [here](https://abelsiqueira.github.io/cm106-2018s1/) in
portuguese.
]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Minicurso de Julia no IX Simpósio de Análise Numérica e Otimização da UFPR]]></title>
            <link>https://abelsiqueira.github.io/blog/2017-02-20-minicurso-de-julia-no-ix-simposio-de-analise-numerica-e-otimizacao-da-ufpr</link>
            <guid>https://abelsiqueira.github.io/blog/2017-02-20-minicurso-de-julia-no-ix-simposio-de-analise-numerica-e-otimizacao-da-ufpr</guid>
            <pubDate>Mon, 20 Feb 2017 00:00:00 GMT</pubDate>
            <content:encoded><![CDATA[
# Minicurso de Julia no IX Simpósio de Análise Numérica e Otimização da UFPR

Hoje ministrarei mais um minicurso de Julia na UFPR.
Desta vez será no IX Simpósio de Análise Numérica e Otimização da UFPR.

Por enquanto, deixo esta página apenas com o link para o notebook que utilizarei:
[aqui](https://github.com/abelsiqueira/julia-simposio2017).
]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[NLPModels.jl and CUTEst.jl: Constrained optimization]]></title>
            <link>https://abelsiqueira.github.io/blog/2017-02-17-nlpmodelsjl-and-cutestjl-constrained-optimization</link>
            <guid>https://abelsiqueira.github.io/blog/2017-02-17-nlpmodelsjl-and-cutestjl-constrained-optimization</guid>
            <pubDate>Fri, 17 Feb 2017 00:00:00 GMT</pubDate>
            <content:encoded><![CDATA[
# NLPModels.jl and CUTEst.jl&#58; Constrained optimization

This is a continuation of [this
post](https://abelsiqueira.github.io{{local_prefix}}nlpmodelsjl-cutestjl-and-other-nonlinear-optimization-packages-on-julia/).
And again, you can follow the commands of this post in the
[asciinema](https://asciinema.org/a/103654).

If you followed along last post, you should know the basics of our
NLPModels API, including CUTEst access.

One thing I didn't explore, though, was constrained problems.
It'd complicate too much.

However, now that we know how to handle the basics, we can move to the
advanced.

**Nonlinear Programming format**

The NLPModels internal structure is based on the CUTEst way of storing a
problem.
We use the following form for the optimization problem:

$$
\begin{align}
\min \quad & f(x) \\
s.t. \quad & c_L \leq c(x) \leq c_U \\
& \ell \leq x \leq u\end{align}
$$

Given an `AbstractNLPModel` named `nlp`, the values for $\ell$, $u$, $c_L$ and
$c_U$ are stored in an `NLPModelMeta` structure, and can be accessed by
through `nlp.meta`.

Let's look back at the simple Rosenbrock problem of before.

```
using NLPModels

f(x) = (x[1] - 1)^2 + 100*(x[2] - x[1]^2)^2
x0 = [-1.2; 1.0]
nlp = ADNLPModel(f, x0)
print(nlp.meta)
```

You should be seeing this:

```
Minimization problem Generic
nvar = 2, ncon = 0 (0 linear)
lvar = -Inf  -Inf
uvar = Inf  Inf
lcon = ∅
ucon = ∅
x0 = -1.2  1.0
y0 = ∅
nnzh = 4
nnzj = 0
```

Although the meaning of these values is reasonably straigthforward, I'll explain a bit.

- `nvar` is the number of variables in a problem;
- `ncon` is the number of constraints, without counting the bounds;
- `lvar` is the vector $\ell$, the lower bounds on the variables;
- `uvar` is the vector $u$, the upper bounds on the variables;
- `lcon` is the vector $c_L$, the lower bounds of the constraints function;
- `ucon` is the vector $c_U$, the upper bounds of the constraints function;
- `x0` is the initial approximation to the solution, aka the starting point;
- `y0` is the initial approximation to the Lagrange multipliers;
- `nnzh` is the number of nonzeros on the Hessian¹;
- `nnzj` is the number of nonzeros on the Jacobian¹;

_¹ `nnzh` and `nnzj` are not consistent between models, because some consider the dense matrix, and for the Hessian, some consider only the triangle. However, if you're possibly considering using `nnzh`, you're probably looking for `hess_coord` too, and `hess_coord` returns with the correct size._

These values can be accessed directly as fields in `meta` with the same name above.

```
nlp.meta.ncon
nlp.meta.x0
nlp.meta.lvar
```

**Bounds**

Now, let's create a bounded problem.

```
nlp = ADNLPModel(f, x0, lvar=zeros(2), uvar=[0.4; 0.6])
print(nlp.meta)
```

Now the bounds are set, and you can access them with

```
nlp.meta.lvar
nlp.meta.uvar
```

That's pretty much it. For `SimpleNLPModel`, it's the same thing.
`MathProgNLPModel` inherits the bounds, as expected:

```
using JuMP

jmp = Model()
u = [0.4; 0.6]
@variable(jmp, 0 <= x[i=1:2] <= u[i], start=(x0[i]))
@NLobjective(jmp, Min, (x[1] - 1)^2 + 100*(x[2] - x[1]^2)^2)
mpbnlp = MathProgNLPModel(jmp)
print(mpbnlp.meta)
```

For CUTEst, there is no differentiation on creating a problem with bounds or
not, since it uses the internal description of the problem.
For instance, `HS4` is a bounded problem.

```
using CUTEst

clp = CUTEstModel("HS4")
print(clp.meta)
finalize(clp)
```

Notice that it can happen that one or more of the variables is unlimited
(lower, upper or both). This is represented by the value `Inf` in Julia.
This should be expected since the unconstrained problem already used these
`Inf` values.

On the other hand, it could happen that $\ell_i = u_i$, in which case the
variable is fixed, or that $\ell_i > u_i$, in which case the variable (and the
problem) is infeasible.
Note that `NLPModels` only creates the model, it doesn't check whether it is
feasible or not, even in this simple example. That said, CUTEst shouldn't have
any infeasible variable.

Furthermore, all these types of bounds can be accessed from `meta`. Notice that
there are 6 possible situations:

- Free variables, stored in `meta.ifree`;
- Fixed variables, stored in `meta.ifix`;
- Variables bounded below, stored in `meta.ilow`;
- Variables bounded above, stored in `meta.iupp`;
- Variables bounded above and below, stored in `meta.irng`;
- Infeasible variables, stored in `meta.iinf`.

Here is one example with one of each of them

```
nlp = ADNLPModel(x->dot(x,x), zeros(6),
  lvar = [-Inf, -Inf, 0.0, 0.0, 0.0,  0.0],
  uvar = [ Inf,  1.0, Inf, 1.0, 0.0, -1.0])
nlp.meta.ifree
nlp.meta.ifix
nlp.meta.ilow
nlp.meta.iupp
nlp.meta.irng
nlp.meta.iinf
```

**Constraints**

Constraints are stored in NLPModels following in the format $c_L \leq c(x) \leq c_U$.
That means that an equality constraint happens when $c_{L_j} = c_{U_j}$.
Let's look at how to create a problem with constraints.

For `ADNLPModel`, you need to pass three keywords arguments: `c`, `lcon` and `ucon`,
which represent $c(x)$, $c_L$ and $c_U$, respectively.
For instance, the problem

$$
\begin{align}
\min \quad & x_1^2 + x_2^2 \\
s.t. \quad & x_1 + x_2 = 1
\end{align}
$$

is created by doing

```
c(x) = [x[1] + x[2] - 1]
lcon = [0.0]
ucon = [0.0]
nlp = ADNLPModel(x->dot(x,x), zeros(2), c=c, lcon=lcon, ucon=ucon)
```

or alternatively, if you don't want the intermediary functions

```
nlp = ADNLPModel(x->dot(x,x), zeros(2), c=x->[x[1]+x[2]-1], lcon=[0.0], ucon=[0.0])
```

Another possibility is to do

```
nlp = ADNLPModel(x->dot(x,x), zeros(2), c=x->[x[1]+x[2]], lcon=[1.0], ucon=[1.0])
```

Personally, I prefer the former.

For inequalities, you can have only lower, only upper, and both.
The commands

```
nlp = ADNLPModel(x->dot(x,x), zeros(2),
  c=x->[x[1] + x[2]; 3x[1] + 2x[2]; x[1]*x[2]],
  lcon = [-1.0; -Inf; 1.0],
  ucon = [Inf;   3.0; 2.0])
```

implement the problem

$$
\begin{align}
\min \quad & x_1^2 + x_2^2 \\
s.t. \quad & x_1 + x_2 \geq -1 \\
& 3x_1 + 2x_2 \leq 3 \\
& 1 \leq x_1x_2 \leq 2.
\end{align}
$$

Again, the types of constraints can be accessed in `meta`, through
`nlp.meta.jfix`, `jfree`, `jinf`, `jlow`, `jrng` and `jupp`.
Notice if you forget to set `lcon` and `ucon`, there will be no
constraints, even though `c` is set. This is because the number of
constraints is taken from the lenght of these vectors.

Now, to access these constraints, let's consider this simple problem.

```
nlp = ADNLPModel(f, x0, c=x->[x[1]*x[2] - 0.5], lcon=[0.0], ucon=[0.0])
```

The function `cons` return $c(x)$.

```
cons(nlp, nlp.meta.x0)
```

The function `jac` returns the Jacobian of $c$. `jprod` and `jtprod` the
Jacobian product times a vector, and `jac_op` the LinearOperator.

```
jac(nlp, nlp.meta.x0)
jprod(nlp, nlp.meta.x0, ones(2))
jtprod(nlp, nlp.meta.x0, ones(1))
J = jac_op(nlp, nlp.meta.x0)
J * ones(2)
J' * ones(1)
```

To get the Hessian we'll use the same functions as the unconstrained case,
with the addition of a keyword parameter `y`.

```
y = [1e4]
hess(nlp, nlp.meta.x0, y=y)
hprod(nlp, nlp.meat.x0, ones(2))
H = hess_op(nlp, nlp.meta.x0, y=y)
H * ones(2)
```

If you want to ignore the objective function, or scale it by some value,
you can use the keyword parameter `obj_weight`.

```
s = 0.0
hess(nlp, nlp.meta.x0, y=y, obj_weight=s)
hprod(nlp, nlp.meat.x0, ones(2), obj_weight=s)
H = hess_op(nlp, nlp.meta.x0, y=y, obj_weight=s)
H * ones(2)
```

Check the
[API](http://juliasmoothoptimizers.github.io/NLPModels.jl/stable/api.html)
for more details.

We can also create a constrained JuMP model.

```
x0 = [-1.2; 1.0]
jmp = Model()
@variable(jmp, x[i=1:2], start=(x0[i]))
@NLobjective(jmp, Min, (x[1] - 1)^2 + 100*(x[2] - x[1]^2)^2)
@NLcontraint(jmp, x[1]*x[2] == 0.5)
mpbnlp = MathProgNLPModel(jmp)
cons(mpbnlp, mpbnlp.meta.x0)
jac(mpbnlp, mpbnlp.meta.x0)
hess(mpbnlp, mpbnlp.meta.x0, y=y)
```

And again, the access in CUTEst problems is the same.

```
clp = CUTEstModel("BT1")
cons(clp, clp.meta.x0)
jac(clp, clp.meta.x0)
hess(clp, clp.meta.x0, y=clp.meta.y0)
finalize(clp)
```

**Convenience functions**

There are some convenience functions to check whether a problem has only
equalities, only bounds, etc.
For clarification, we're gonna say function constraint to refer to constraints that are not bounds.

- `has_bounds`: Returns `true` is variable has bounds.
- `bound_constrained`: Returns `true` if `has_bounds` and no function
  constraints;
- `unconstrained`: No function constraints nor bounds;
- `linearly_constrained`: There are function constraints, and they are
  linear; _obs: even though a `bound_constrained` problem is linearly
  constrained, this will return false_.
- `equality_constrained`: There are function constraints, and they are all equalities;
- `inequality_constrained`: There are function constraints, and they are all inequalities;

**Example solver**

Let's implement a "simple" solver for constrained optimization.
Our solver will loosely follow the Byrd-Omojokun implementation of

> M. Lalee, J. Nocedal, and T. Plantenga. **On the implementation of an algorithm for large-scale equality constrained optimization**. SIAM J. Optim., Vol. 8, No. 3, pp. 682-706, 1998.

```
function solver(nlp :: AbstractNLPModel)
  if !equality_constrained(nlp)
    error("This solver is for equality constrained problems")
  elseif has_bounds(nlp)
    error("Can't handle bounds")
  end

  x = nlp.meta.x0

  fx = obj(nlp, x)
  cx = cons(nlp, x)

  ∇fx = grad(nlp, x)
  Jx = jac_op(nlp, x)

  λ = cgls(Jx', -∇fx)[1]
  ∇ℓx = ∇fx + Jx'*λ
  norm∇ℓx = norm(∇ℓx)

  Δ = max(0.1, min(100.0, 10norm∇ℓx))
  μ = 1
  v = zeros(nlp.meta.nvar)

  iter = 0
  while (norm∇ℓx > 1e-4 || norm(cx) > 1e-4) && (iter < 10000)
    # Vertical step
    if norm(cx) > 1e-4
      v = cg(Jx'*Jx, -Jx'*cx, radius=0.8Δ)[1]
      Δp = sqrt(Δ^2 - dot(v,v))
    else
      fill!(v, 0)
      Δp = Δ
    end

    # Horizontal step
    # Simplified to consider only ∇ℓx = proj(∇f, Nu(A))
    B = hess_op(nlp, x, y=λ)
    B∇ℓx = B * ∇ℓx
    gtBg = dot(∇ℓx, B∇ℓx)
    gtγ = dot(∇ℓx, ∇fx + B * v)
    t = if gtBg <= 0
      norm∇ℓx > 0 ? Δp/norm∇ℓx : 0.0
    else
      t = min(gtγ/gtBg, Δp/norm∇ℓx)
    end

    d = v - t * ∇ℓx

    # Trial step acceptance
    xt = x + d
    ft = obj(nlp, xt)
    ct = cons(nlp, xt)
    γ = dot(d, ∇fx) + 0.5*dot(d, B * d)
    θ = norm(cx) - norm(Jx * d + cx)
    normλ = norm(λ, Inf)
    if θ <= 0
      μ = normλ
    elseif normλ > γ/θ
      μ = min(normλ, 0.1 + γ/θ)
    else
      μ = 0.1 + γ/θ
    end
    Pred = -γ + μ * θ
    Ared = fx - ft + μ * (norm(cx) - norm(ct))

    ρ = Ared/Pred
    if ρ > 1e-2
      x .= xt
      fx = ft
      cx .= ct
      ∇fx = grad(nlp, x)
      Jx = jac_op(nlp, x)
      λ = cgls(Jx', -∇fx)[1]
      ∇ℓx = ∇fx + Jx'*λ
      norm∇ℓx = norm(∇ℓx)
      if ρ > 0.75 && norm(d) > 0.99Δ
        Δ *= 2.0
      end
    else
      Δ *= 0.5
    end

    iter += 1
  end

  return x, fx, norm∇ℓx, norm(cx)
end
```

Too loosely, in fact.

- The horizontal step computes only the Cauchy step;
- No special updates;
- No second-order correction;
- No efficient implementation beyond the easy-to-do.

To test how good it is, let's run on the Hock-Schittkowski constrained problems.

```
function runcutest()
  problems = filter(x->contains(x, "HS") && length(x) <= 5, CUTEst.select(only_free_var=true, only_equ_con=true))
  sort!(problems)
  @printf("%-7s  %15s  %15s  %15s\n",
          "Problem", "f(x)", "‖∇ℓ(x,λ)‖", "‖c(x)‖")
  for p in problems
    nlp = CUTEstModel(p)
    try
      x, fx, nlx, ncx = solver(nlp)
      @printf("%-7s  %15.8e  %15.8e  %15.8e\n", p, fx, nlx, ncx)
    catch
      @printf("%-7s  %s\n", p, "failure")
    finally
      finalize(nlp)
    end
  end
end
```

I'm gonna print the output of this one, so you can compare it with yours.

```
Problem             f(x)        ‖∇ℓ(x,λ)‖           ‖c(x)‖
HS26      5.15931251e-07   9.88009545e-05   5.24359322e-05
HS27      4.00000164e-02   5.13264248e-05   2.26312672e-09
HS28      7.00144545e-09   9.46563681e-05   2.44249065e-15
HS39     -1.00000010e+00   1.99856691e-08   1.61607518e-07
HS40     -2.50011760e-01   4.52797064e-05   2.53246505e-05
HS42      1.38577292e+01   5.06661945e-05   5.33092868e-05
HS46      3.56533430e-06   9.98827045e-05   8.00086215e-05
HS47      3.53637757e-07   9.71339790e-05   7.70496596e-05
HS48      4.65110036e-10   4.85457139e-05   2.27798719e-15
HS49      3.14248189e-06   9.94899395e-05   2.27488138e-13
HS50      1.36244906e-12   2.16913725e-06   2.90632554e-14
HS51      1.58249170e-09   8.52213221e-05   6.52675179e-15
HS52      5.32664756e+00   3.35626559e-05   3.21155766e-14
HS56     -3.45604528e+00   9.91076239e-05   3.14471179e-05
HS6       5.93063756e-13   6.88804464e-07   9.61311292e-06
HS61     -1.43646176e+02   1.06116455e-05   1.80421875e-05
HS7      -1.73205088e+00   1.23808109e-11   2.60442422e-07
HS77      2.41501014e-01   8.31210333e-05   7.75367223e-05
HS78     -2.91972281e+00   2.27102179e-05   2.88776440e-05
HS79      7.87776482e-02   4.77319205e-05   7.55827729e-05
HS8      -1.00000000e+00   0.00000000e+00   2.39989802e-06
HS9      -5.00000000e-01   1.23438228e-06   3.55271368e-15
```

If you compare against the Hock-Schitkowski paper, you'll see that
the method converged for all 22 problems.
Considering our simplifications, this is a very exciting.

That's all for now. Use our RSS feed to keep updated.
]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[NLPModels.jl, CUTEst.jl and other Nonlinear Optimization Packages on Julia]]></title>
            <link>https://abelsiqueira.github.io/blog/2017-02-07-nlpmodelsjl-cutestjl-and-other-nonlinear-optimization-packages-on-julia</link>
            <guid>https://abelsiqueira.github.io/blog/2017-02-07-nlpmodelsjl-cutestjl-and-other-nonlinear-optimization-packages-on-julia</guid>
            <pubDate>Tue, 07 Feb 2017 00:00:00 GMT</pubDate>
            <content:encoded><![CDATA[
# NLPModels.jl, CUTEst.jl and other Nonlinear Optimization Packages on Julia

A couple of weeks ago me and Professor [Dominique Orban](https://dpo.github.io) have finally made a release of
CUTEst.jl, a wrapper for the CUTEst repository of problems for nonlinear
optimization (which I've mentioned before).
Along with this release, we've done a release of NLPModels.jl, the underlying
package. I think it's time I explain a little about these packages, others,
and how to use them together.
If you want to see the output of the commands, you can open
[this ASCIInema](https://asciinema.org/a/102371)
side by side.

_Obs.: Tutorial using Julia 0.5.0_

_Edit: Second part is
[here](https://abelsiqueira.github.io{{local_prefix}}nlpmodelsjl-and-cutestjl-constrained-optimization/)._

**JuliaSmoothOptimizers**
[![JuliaSmoothOptimizers logo](https://juliasmoothoptimizers.github.io/assets/logo.png){: .img-view }](https://juliasmoothoptimizers.github.io)

Most packages mentioned here will be a part of the JuliaSmoothOptimizers (JSO)
organization. There are more packages in the organization that I won't mention here, but you should check it out.

**NLPModels.jl**

NLPModels is a package for creating Nonlinear Optimization Models. It is
focused on the needs of the solver writer, such as the ability to write one
solver that works on many models.
The package defines a few models, and there are others on the horizon.
The ones already done are:

- **ADNLPModel**: A model with automatic differentiation;
- **MathProgNLPModel**: A model for [MathProgBase](https://github.com/JuliaOpt/MathProgBase.jl)/[JuMP](http://github.com/JuliaOpt/JuMP.jl) conversion, whose utility will be shown below (obs: MPB and JuMP are packages from the JuliaOpt organization);
- **SimpleNLPModel**: A model in which nothing is automatic, i.e., all functions have to be provided by the user.
- **SlackModel**: A model that changes all inequalities to equalities adding extra variables;
- **LBFGSModel** and **LSR1Model**: Models that create quasi-Newton models from another model.

The first two models are designed to be easy to use; the third is useful for
efficient model creation in specific cases; the last ones are utility models.

Let's begin by installing NLPModels.jl, and a couple of optional requirements.

```
Pkg.add("NLPModels.jl")
Pkg.add("JuMP.jl") # Installs ForwardDiff also.
```

This should install version 0.1.0. After that, just do

```
using NLPModels
```

Now, let's create a simple function: Rosenbrock's.

```
f(x) = (x[1] - 1)^2 + 100*(x[2] - x[1]^2)^2
```

The Rosenbrock problem traditionally starts from $(-1.2,1.0)$.

```
x0 = [-1.2; 1.0]
```

Now, we are ready to create the problem.

```
adnlp = ADNLPModel(f, x0)
```

Now, we can access the function and derivatives using the [NLPModels API](https://juliasmoothoptimizers.github.io/NLPModels.jl/stable/api.html)

```
obj(adnlp, adnlp.meta.x0)
grad(adnlp, adnlp.meta.x0)
hess(adnlp, adnlp.meta.x0)
objgrad(adnlp, adnlp.meta.x0)
hprod(adnlp, adnlp.meta.x0, ones(2))
H = hess_op(adnlp, adnlp.meta.x0)
H * ones(2)
```

At this point, we can't differentiate many things from simply using
`ForwardDiff` interface directly, but two things stand out: `objgrad` returns
both functions at once, and `hess_op` returns a
[LinearOperator](https://github.com/JuliaSmoothOptimizers/LinearOperators.jl),
another structure created in JuliaSmoothOptimizers.
This one defines a linear operator, extending Julia matrices in the sense that if

```
using LinearOperators
n = 100
A = rand(n, n)
B = rand(n, n)
opA = LinearOperator(A)
opB = LinearOperator(B)
v = rand(n)
```

then `(A * B) * v` computes the matrix product, whereas `(opA * opB) * v` won't.
Furthermore, the linear operator can be created from the functions
`v->Mp(v)` and `v->Mtp(v)`, defining the product of the linear operator times a vector and its transpose times a vector.

```
T = LinearOperator(2, 2, # sizes
                   false, false,
                   v->[-v[2]; v[1]], v->[v[2]; -v[1]])
v = rand(2)
T * v
T' * v
```

_Obs: In the `ADNLPModel` case, `hess_op` returns a linear operator that is actually
computing the matrix, but this is a issue to be tackled on the future (PRs
welcome). But we'll be back with uses for `hess_op` soon._

The next model is the `MathProgNLPModel`. This model's main use is the `JuMP`
modelling language. This is very useful for more elaborate writing, specially
with constraints. It does create a little more overhead though, so keep that
in mind.

```
using JuMP
jmp = Model()
@variable(jmp, x[i=1:2], start=(x0[i])) # x0 from before
@NLobjective(jmp, Min, (x[1] - 1)^2 + 100*(x[2] - x[1]^2)^2)
mpbnlp = MathProgNLPModel(jmp)
```

Try the commands again.

```
obj(mpbnlp, mpbnlp.meta.x0)
grad(mpbnlp, mpbnlp.meta.x0)
hess(mpbnlp, mpbnlp.meta.x0)
objgrad(mpbnlp, mpbnlp.meta.x0)
hprod(mpbnlp, mpbnlp.meta.x0, ones(2))
H = hess_op(mpbnlp, mpbnlp.meta.x0)
H * ones(2)
```

It should be pretty much the same, though there is a little difference in `hess`.
JuMP creates the sparse Hessian, which is better, from a computational point of
view.

Notice how the commands are the same. I've actually copy-pasted the commands
from above.
This allows the write of a solver in just a couple of commands.
For instance, a simple **Newton method**.

```
function newton(nlp :: AbstractNLPModel)
  x = nlp.meta.x0
  fx = obj(nlp, x)
  gx = grad(nlp, x)
  ngx = norm(gx)
  while ngx > 1e-6
    Hx = hess(nlp, x)
    d = -gx
    try
      G = chol(Hermitian(Hx, :L)) # Make Cholesky work on lower-only matrix.
      d = -G\(G'\gx)
    catch e
      if !isa(e, Base.LinAlg.PosDefException); rethrow(e); end
    end
    t = 1.0
    xt = x + t * d
    ft = obj(nlp, xt)
    while ft > fx + 0.5 * t * dot(gx, d)
      t *= 0.5
      xt = x + t * d
      ft = obj(nlp, xt)
    end
    x = xt
    fx = ft
    gx = grad(nlp, x)
    ngx = norm(gx)
  end
  return x, fx, ngx
end
```

And we run in the problems with

```
newton(adnlp)
newton(mpbnlp)
```

_Write once, use on problems from different sources._

Now, to have more fun, let's get another package:
[OptimizationProblems.jl](https://github.com/JuliaSmoothOptimizers/OptimizationProblems.jl).
This package doesn't have a release yet, so we have to clone it:

```
Pkg.clone("https://github.com/JuliaSmoothOptimizers/OptimizationProblems.jl")
```

What we have here is a collection of JuMP models implementing some of the
CUTEst problems. Together with `NLPModels.jl`, we have a good opportunity to test our Newton implementation.

```
using OptimizationProblems

x, fx, ngx = newton(MathProgNLPModel( rosenbrock() ))
x, fx, ngx = newton(MathProgNLPModel( dixmaanj() ))
x, fx, ngx = newton(MathProgNLPModel( brownbs() ))
```

_An issue with OptimizationProblems is that it still doesn't have a way to get
all unconstrained problems, for instance. (PRs are welcome)._

So far we used 3 packages from JSO: `NLPModels.jl`, `LinearOperators.jl` and `OptimizationProblems.jl`. It's time to meet another important package.

**CUTEst.jl**

CUTEst, the Constrained and Unconstrained Testing Environment with safe
threads, is a package written in Fortran providing over a thousand problems to
allow testing of Nonlinear Programming solvers. However, CUTEst is hard to use
by first-timers. Just installing it was already hard.
CUTEst.jl provides an interface for CUTEst that is simple to install and use
(comparing to the original).

_Obs.: CUTEst.jl does not work on Windows for now. In fact, there is no plan to
make it work on Windows because "people interested in doing it"∩"people capable
of doing it" = ∅, as far as we've looked. If you are in this set, PRs are
welcome._

To install CUTEst.jl you need to install something manually. Unfortunately,
this is specific for each system. Except for OSX, actually, which is using
[homebrew-cutest](https://github.com/optimizers/homebrew-cutest).

For Linux users, check out [this
page](http://juliasmoothoptimizers.github.io/CUTEst.jl/latest/#Installing-1).
Essentially, we need `libgfortran.so` in a visible place. And it's especially
annoying that some distritions don't put it in a visible place.

With that done, enter

```
Pkg.add("CUTEst")
```

which should install CUTEst.jl 0.1.0.

Yes, it takes some time.

Finally, we start using CUTEst with

```
using CUTEst

nlp = CUTEstModel("ROSENBR")
```

`ROSENBR` is a CUTEst problem, in case you want the list, see
[here](http://www.cuter.rl.ac.uk/Problems/mastsif.html). Keep reading for a way
to select them, though.

Now, let's solve this CUTEst problem with our Newton method.

```
x, fx, ngx = newton(nlp)
```

**Yes, exactly like before!**.

CUTEst is a little more annoying in other aspect also. Like, you can't have two
or more problems open at the same time, and you have to close this problem
before opening a new one. (Again, PRs are welcome).

```
finalize(nlp)
nlp = CUTEstModel("HIMMELBB")
x, fx, ngx = newton(nlp)
finalize(nlp)
```

This allows a simple workflow for writing optimization solvers.

- Write some problems by hand (using `ADNLPModel` or `MathProgNLPModel`);
- Test your solvers with these hand-written problems;
- Repeat last two steps until you believe you're ready to competitive comparison;
- Test with CUTEst problems seamlessly.

Now, let's get back to `hess_op`. Remember that it used Matrix vector products?
Well, CUTEst has separate functions for the product of the Hessian at a point
and a vector. Which means that `hprod` actually computes this product without
having to create the matrix. Which means it is, at least, memory-efficient.
Furthermore, `hess_op` will be created with the `hprod` function, which means
it is also memory-efficient.

Let's look at a huge problem to feel the difference.

```
nlp = CUTEstModel("BOX")
nlp.meta.nvar
```

Let's make a simple comparison

```
function foo1()
  H = hess(nlp, nlp.meta.x0)
  v = ones(nlp.meta.nvar)
  return Hermitian(H, :L) * v
end

function foo2()
  H = hess_op(nlp, nlp.meta.x0)
  v = ones(nlp.meta.nvar)
  return H * v
end

@time w1 = foo1();
@time w2 = foo2();
norm(w1 - w2)
```

Yes, that's a huge difference.

This is a very good reason to use `hess_op` and `hprod`. But let's take a step further.

We can't implement Cholesky using only `hprod`s, so our Newton method would
actually take a long time to reach a solution for the problem above.
To circunvent that, we could try using the Conjugate Gradients Method instead
of Cholesky. This would only use Hessian-vector products.

We arrive on a new package,
[Krylov.jl](https://github.com/JuliaSmoothOptimizers/Krylov.jl), which
implements Krylov methods. In particular, Conjugate Gradients.
This package is also unreleased, so we need to clone it.

```
Pkg.clone("https://github.com/JuliaSmoothOptimizers/Krylov.jl")
```

Consider a simple example

```
using Krylov
A = rand(3,3)
A = A*A'
b = A*ones(3)
cg(A, b)
```

As expected, the system is solver, and the solution is $(1,1,1)$.
But let's do something more.

```
A = -A
cg(A, b)
```

Yes, Krylov does indeed solves the non-positive definite system using Conjugate Gradient.
Well, actually, a variant.

That's not enough tough. Krylov.jl also accepts an additional argument `radius`
to set a trust-region radius.

```
cg(A, b, radius=0.1)
```

Well, as an exercise I suggest you implement a trust-region version of Newton
method, but for now, let's continue with our line-search version.

We know now how `cg` behaves for non-positive definite systems, we can't make
the changes for a new method.

```
function newton2(nlp :: AbstractNLPModel)
  x = nlp.meta.x0
  fx = obj(nlp, x)
  gx = grad(nlp, x)
  ngx = norm(gx)
  while norm(gx) > 1e-6
    Hx = hess_op(nlp, x)
    d, _ = cg(Hx, -gx)
    slope = dot(gx, d)
    if slope >= 0 # Not a descent direction
      d = -gx
      slope = -dot(d,d)
    end
    t = 1.0
    xt = x + t * d
    ft = obj(nlp, xt)
    while ft > fx + 0.5 * t * slope
      t *= 0.5
      xt = x + t * d
      ft = obj(nlp, xt)
    end
    x = xt
    fx = ft
    gx = grad(nlp, x)
    ngx = norm(gx)
  end
  return x, fx, ngx
end
```

Now, running `newton2` on our large problem, we obtain

```
x, fx, ngx = newton2(nlp)
```

Which is the method working very fast. Less that a second here.

---

There is actually another package I'd like to talk about, but it needs some
more work for it to be ready for a release:

**Optimize.jl**

Optimize.jl is a package with solvers. We intend to implement some high quality
solvers in there, but there is actually more to it. We have in there tools to
benchmark packages. These tools should allow the testing of a set of solvers in
a set of problems without much fuss, while creating the comparison information,
including the performance profile.
It also includes, or will include, "parts" of solvers to create your own
solver. Like trust-region and line-search algorithms and auxiliary functions
and types.
Unfortunately, it's not done enough for me to extend on it, and this is already
getting too long.

**End**

I hope you enjoyed this overview of packages.
Subscribe to the RSS feed to keep updated in future tutorials. I intend to talk
about the constrained part of NLPModels soon.
]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Julia Fractal on Julia]]></title>
            <link>https://abelsiqueira.github.io/blog/2017-01-15-julia-fractal-on-julia</link>
            <guid>https://abelsiqueira.github.io/blog/2017-01-15-julia-fractal-on-julia</guid>
            <pubDate>Sun, 15 Jan 2017 00:00:00 GMT</pubDate>
            <content:encoded><![CDATA[
# Julia Fractal on Julia

I wanted a background that would update automatically in some
interesting way, instead of just random images.
After some thought, I decided to use some [Julia
fractals](https://en.wikipedia.org/wiki/Julia_set).

I made a code to create the Julia fractals in the [Julia
language](https://julialang.org), and then some code to run it for a random
point.

The code is [here](https://github.com/abelsiqueira/juliabg), including some
explaining on how to use and install it.

Here are some examples:

![](https://raw.githubusercontent.com/abelsiqueira/juliabg/master/ex1.png)
![](https://raw.githubusercontent.com/abelsiqueira/juliabg/master/ex2.png)
![](https://raw.githubusercontent.com/abelsiqueira/juliabg/master/ex3.png)
![](https://raw.githubusercontent.com/abelsiqueira/juliabg/master/ex4.png)
]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Advent of Code 2016 in Julia]]></title>
            <link>https://abelsiqueira.github.io/blog/2016-12-18-advent-of-code-2016-in-julia</link>
            <guid>https://abelsiqueira.github.io/blog/2016-12-18-advent-of-code-2016-in-julia</guid>
            <pubDate>Sun, 18 Dec 2016 00:00:00 GMT</pubDate>
            <content:encoded><![CDATA[
# Advent of Code 2016 in Julia

Last year I discovered by chance a code competition that happens in the month
of December: The [Advent of Code](http://adventofcode.com).
This event consists of small puzzles release every day of December, up to the
25th. Which amounts to 25 problems! (I can do math).

Last year I tried doing some on Bash, and latter switched to Julia, but never
got to finish them.
This year I decided to use [Julia](https://julialang.org), because I want to
test it for these general problems. Some problems are very hard, and I want
to have the packages at my disposal.

**There is a leaderboard**, which revolves around the first 100 solvers for
each day. Most problems are solved in the first 5 minutes of its release, and
the 100th place is usually less than 15 minutes. The best I got was 30 minutes,
getting the 135th position and zero points. However, I'm usually not trying to
get there because the problems are release at 3am for me (Brazil), and by that
time I'm usually sleeping.

This year I'm trying to follow more closely the dates, managed to solve one by
day until a gigantic workload halted my progress for a week. Yesterday I
returned to these problems and solved the missing ones up until now.

If you like coding, you should give it a try. My code for this year is
available at [GitHub](https://github.com/abelsiqueira/AoC2016), but only open
it if you decided not to play. Be warned that most of my answers were poorly
written because I still want to do it fast, and am not worried about cleaning
of efficiency.

Happy new year, holidays, and coding.
]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Apresentação de Julia no SMNE]]></title>
            <link>https://abelsiqueira.github.io/blog/2016-11-29-smne-2016-julia</link>
            <guid>https://abelsiqueira.github.io/blog/2016-11-29-smne-2016-julia</guid>
            <pubDate>Tue, 29 Nov 2016 00:00:00 GMT</pubDate>
            <content:encoded><![CDATA[
# Apresentação de Julia no SMNE

Nos dias 30 de Novembro à 2 de Dezembro de 2016 acontece o [primeiro Simpósio
de Métodos Numéricos em Engenharia](http://eventos.ufpr.br/smne/SMNE1).
Neste simpósio darei um minicurso sobre Julia.

Deixo aqui o material preliminar para os alunos do minicurso, ou interessados.

- [Notebook (necessário)](/blog/smne-julia.ipynb)
- [exemplo.jl (necessário)](/blog/exemplo.jl)
- [HTML estático do notebook - para quem não conseguiu
  instalar](/blog/smne-julia.html)

Também deixo [aqui](https://pad.riseup.net/p/aCXYqUjz3cCS) o link do Etherpad
para usarmos na aula.

Quem não conseguiu instalar até agora, pode tentar usar o
[JuliaBox](https://juliabox.com/), que roda Julia online. **Não sei se teremos
internet boa o suficiente no evento**.
]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Test Driven Development in Julia]]></title>
            <link>https://abelsiqueira.github.io/blog/2016-03-13-test-driven-development-in-julia</link>
            <guid>https://abelsiqueira.github.io/blog/2016-03-13-test-driven-development-in-julia</guid>
            <pubDate>Sun, 13 Mar 2016 00:00:00 GMT</pubDate>
            <content:encoded><![CDATA[
# Test Driven Development in Julia

First, what is Test Driven Development (TDD)?
Well, I'm not an expert, so don't quote me, but in practice it means that you
develop your code to fulfill tests that you define prior to beginning your work.
You do not define **all** your tests first, though. You define a single test,
and produce code to pass it. Then you define another code, and produce code to
pass both. And so forth until you complete your specification.

This is good because:

- Wherever you stop, you know what is working.
- When you finish, your code already has tests.
- [You don't have to optimize prematurely.](http://c2.com/cgi/wiki?PrematureOptimization)

The steps of TDD can be described as [from
Wikipedia](https://en.wikipedia.org/wiki/Test-driven_development)

1. **Add tests**: These should be useful, _and should fail_.
2. **Run tests**: Verify that the test fails. If not, go back to 1.
3. **Write code**: Write enough code to pass the test.
4. **Run tests**: Verify that **all** tests pass. If some of the tests fail, go back to 3.
5. **Refactor**: Now that everything passes, make the code looks nicer. This is harder for non-seasoned programmers, because it's vague. Essentially, it means removing duplicate code, magic numbers, clarifying names, etc.
6. **Run tests**: Again. Should be done during refactoring, to guarantee you're not breaking anything. But just to be very clear: _your tests should pass at the end of refactoring_.
7. **Repeat**.

This is one way of describing TDD, but there are other. Many others, by the way.
In fact, there are many images describing it, so you can print one and staple it
around.

## Julia

First, we are gonna follow the _package_ layout in Julia.
[This post]({{local_prefix}}/automated-testing/) mentions it at the end.
Basically, we need

- Folder PackageName.jl
  - Folder `src`
  - PackageName.jl
  - Folder `test`
  - runtests.jl
  - README.md
  - LICENSE.md

In our example, we're gonna write a program to convert Roman numbers to decimal,
and vice-versa.
This was inspired by [this
site](http://codingdojo.org/cgi-bin/index.pl?KataRomanNumerals).

**Important: You should use git, but I'll skip it here**

Let's begin writing the outline of the project

```
mkdir RomanNumerals.jl
cd RomanNumerals
mkdir src test
```

```
# File src/RomanNumerals.jl
module RomanNumerals

end
```

```
# File test/runtests.jl
using RomanNumerals

include("test_digits.jl")
```

This defines the building blocks. Note that `test_digits.jl` does not exist. We're
gonna create it to test the individuals digits.

Our testing environment will consist of having a terminal open at all
times at the root of this project. Our testing command will be

```
julia -L src/RomanNumerals.jl test/runtests.jl
```

There are different ways to issue the same command, but this is locally good.

## Tests

Julia comes with a `Base.Test` package, which is the least you should use.
For all basic things it is enough. It provides the `@test` macro, which you can
use as

```
using Base.Test
@test 1 == 1 # This will pass
@test 1 == 0 # This will fail
```

We're gonna go a step beyong and use
[FactCheck.jl](https://github.com/JuliaLang/FactCheck.jl).
This provides more information about the tests.

We're gonna implement the function `roman_to_dec` which receives a string with
roman numerals and returns the decimal equivalent of the number.
With `FactCheck`, our first test will be

```
# File test/test_digits.jl
using FactCheck

facts("Testing digits") do
  @fact roman_to_dec("I") --> 1
end
```

When we run our test, we'll get

```
Testing digits
  Error :: (line:-1)
    Expression: roman_to_dec("I") --> 1
    UndefVarError: roman_to_dec not defined
      ...
```

Look, `roman_to_dec not defined`. Well, let's define it.

```
# File src/RomanNumerals.jl
...
export roman_to_dec

function roman_to_dec(s)
end
...
```

Running again, we get an even better message

```
Testing digits
  Error :: (line:-1) :: fact was false
    Expression: roman_to_dec("I") --> 1
      Expected: 1
      Occurred: nothing
Out of 1 total fact:
  Failed:   1
```

Expected 1, nothing ocurred. Well, that's easy.

```
# File src/RomanNumerals.jl
...
function roman_to_dec(s)
  return 1
end
...
```

```
Testing digits
1 fact verified
```

Done. We're successful. Rejoice. Back to work.

We've written a test, we've tested it, we've written code to fix it, we tested
it. Not much to refactor, this is a silly example.

Repeat. Let's improve the tests.

```
# File test/test_digits.jl
...
facts("Testing digits") do
  @fact roman_to_dec("I") --> 1
  @fact roman_to_dec("V") --> 5
end
```

Running, we obtain

```
Testing digits
  Error :: (line:-1) :: fact was false
    Expression: roman_to_dec("V") --> 5
      Expected: 5
      Occurred: 1
Out of 2 total fact:
  Verified: 1
  Failed:   1
```

Now, that's better. Improving the code.

```
# File src/RomanNumerals.jl
...
function roman_to_dec(s)
  if s == "I"
    return 1
  else
    return 5
  end
end
...
```

This too will pass. Notice that this example is very silly. It is instructional,
of course. On a real application, you could start with all digits at once, for
instance.

More tests and solutions:

```
# File test/test_digits.jl
...
facts("Testing digits") do
  @fact roman_to_dec("I") --> 1
  @fact roman_to_dec("V") --> 5
  @fact roman_to_dec("X") --> 10
end
```

```
# File src/RomanNumerals.jl
...
function roman_to_dec(s)
  if s == "I"
    return 1
  elseif s == "V"
    return 5
  else
    return 10
  end
end
...
```

Now we can refactor, because it's getting very ugly.

```
# File src/RomanNumerals.jl
...
const digits = Dict("I"=>1, "V"=>5, "X"=>10)

function roman_to_dec(s)
  return digits[s]
end
...
```

We can also refactor the test.

```
# File test/test_digits.jl
...
facts("Testing digits") do
  for (digit,value) in [("I",1), ("V",5), ("X",10)]
    @fact roman_to_dec(digit) --> value
  end
end
```

Test. Now we can add more tests for digits, and it will be much easier (because
it's refactored) to both create the test and to solve it.

Understanding the logic now, you can add all the rest of the digits at once.
**Remember to test before start fixing,** even though is very easy now.
This could be a breaking moment on your code. If, when trying to fix it, you
realize it's not as simple as you expected. Remove the test, and add a smaller
one. At this time it will be very useful to have been using git.

```
# File test/test_digits.jl
...
facts("Testing digits") do
  for (digit,value) in [("I",1), ("V",5), ("X",10), ("L",50), ("C",100),
      ("D",500), ("M",1000)]
    @fact roman_to_dec(digit) --> value
  end
end
```

```
# File src/RomanNumerals.jl
...
const digits = Dict("I"=>1, "V"=>5, "X"=>10, "L"=>50, "C"=>100, "D"=>500,
  "M"=>1000)
...
```

## Next test

We've completed a test. Let's do the next.

```
# File test/runtests.jl
using RomanNumerals

include("test_digits.jl")
include("test_double_digits.jl")
```

Double digits are more complex that single digits (by at least at factor of 2?
:) ). Let's break it down using `context`.

```
# File test/test_double_digits.jl
using FactCheck

facts("Testing double digits") do
  context("Repeated digits") do
    @fact roman_to_dec("II") --> 2
  end
end
```

Testing this will fail (as it should), with a `KeyError: II not found`, because
we're using the dictionary, and "II" is not in it.

Before reading the solution, try to fix it yourself. There are many ways to do
it.

```
# File src/RomanNumerals.jl
...
function roman_to_dec(s)
  dec = 0
  for i = 1:length(s)
    dec += digits[s[i:i]]
  end
  return dec
end
```

This fixes it. Now to refactor. You may have noticed that `s[i]` does not work
inside `digits`. That is because julia differentiates characters and single
digits strings (like C, unlike Python). One refactor option is to change the
dictionary to use chars.
Another option is to use a better variable instead of s, since it start to
become a nuisance to read.
Yet another, is to use another way to make the sum.

Since this post explains the usage of TDD, it ends here.
You can continue with this problem until you can make a complete conversor of
roman to decimal.
]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Minicurso de Julia para Otimização]]></title>
            <link>https://abelsiqueira.github.io/blog/2016-02-28-minicurso-de-julia-para-otimizacao</link>
            <guid>https://abelsiqueira.github.io/blog/2016-02-28-minicurso-de-julia-para-otimizacao</guid>
            <pubDate>Sun, 28 Feb 2016 00:00:00 GMT</pubDate>
            <content:encoded><![CDATA[
# Minicurso de Julia para Otimização

Semana passada, nos dias 22 a 24 de Fevereiro aconteceu o VIII Simpósio de
Análise Numérica e Otimização.
Nesse evento, eu apresentei um minicurso de Julia com foco em Otimização não
Linear.
Entre 10 e 15 pessoas participaram no total, entre alunos de graduação, de
pós-graduação, e professores.
O objetivo principal era apresentar a linguagem e alguns detalhes importante
para otimização não linear, o que foi obtido.
Por outro lado, as aulas foram mais longas do que deveriam, e o conteúdo ficou
muito esparso, pois tive que variar entre o básico e uma aplicação mais
avançada.
Para o próximo workshop, provavelmente focarei na introdução, que foi a primeira
aula.

O conteúdo pode ser obtido
[aqui](https://github.com/abelsiqueira/julia-workshop), e consiste de três
[notebooks em Jupyter](http://jupyter.org/), uma introdução e outros arquivos.

Para rodar os notebooks, você precisa instalar o
[Jupyter](http://jupyter.org/), o [Julia](http://julialang.org/) e o
[IJulia](https://github.com/JuliaLang/IJulia.jl).
Alternativamente, use o [JuliaBox](https://juliabox.org/) para rodá-los online.

Além disso, para a segunda aula em diante você precisa do CUTEst, que pode ser
instalado seguindo
[este post]({{local_prefix}}/installing-cutest-and-cutest.jl/).

Minha recomendação de editor é o [Atom](http://atom.io), com os plugins
`language-julia` e `latex-completions`.

Para uma experiência com testes automatizados, veja também
[este post]({{local_prefix}}/automated-testing/).
]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Automated testing]]></title>
            <link>https://abelsiqueira.github.io/blog/2016-02-21-automated-testing</link>
            <guid>https://abelsiqueira.github.io/blog/2016-02-21-automated-testing</guid>
            <pubDate>Sun, 21 Feb 2016 00:00:00 GMT</pubDate>
            <content:encoded><![CDATA[
# Automated testing

We're gonna learn how to make a test for your Julia code that runs whenever you
publish it online. We're gonna use

- [GitHub](http://github.com) to store the code;
- [Travis CI](http://travis-ci.com) to run your tests;
- [Coveralls.io](http://coveralls.io) to verify which lines of code your test are missing.

Alternatively, for a open source alternative, see [GitLab](http://gitlab.com),
which I don't know enough yet.

---

Let's make a litte code to solve a linear system, paying attention to the
problems it may arise, like incorrect dimensions, underdetermined and
overdetermined systems, singular matrices, etc.
And we'll be using the factorizations, but not `\`.

## The math

A simple (not cheap) way to do it is using Singular Value Decomposition (SVD).
We have

$$
  A = U\Sigma V^T = \sum_{i=1}^r \sigma_i u_i v_i^T.
$$

where $r$ is the rank of $A$.
Since the columns of $V$ form a basis for $\mathbb{R}^n$ (where $x$ resides),
then

$$
  x = \sum_{j=1}^n \alpha_j v_j.
$$

Now, we have

$$
  Ax = \sum_{i=1}^r \sum_{j=1}^n \sigma_i \alpha_j u_i v_i^Tv_j
     = \sum_{i=1}^r \sigma_i \alpha_i u_i
$$

If the system has a solution, that is $Ax = b$, then we multiply by $u_j^T$,
obtaining

$$
  u_j^TAx = \sum_{i=1}^r \sigma_i \alpha_i u_j^Tu_i
          = \sigma_j\alpha_j = u_j^Tb
$$

Thus, $ \alpha_j = \dfrac{u_j^Tb}{\sigma_j} $.
If the system doesn't have a solution, this still holds. I'll leave the steps to
you.

If $r < n$, then $\alpha$ has undetermined values. However, when that's the
case, the solution we'll use is the one with the smallest norm, that is, the one
that minimizes $\Vert x\Vert$.
Since $v_i$ are orthonormal, then
$$ \Vert x\Vert^2 = \sum\_{i=1}^n \alpha_i^2.$$
So, in order to minimize the norm of x, we use $\alpha_i = 0$ for $i > r$.

## The code

We'll store the code on
[this](http://github.com/abelsiqueira/BlogAutomatedTesting.jl) GitHub
repository.
Note, however, that it will point to the completed version.

A possible implementation of our code is as follows:

```
# File src/solve_linear.jl
{% include_relative {{ page.name }}/solve_linear1.jl %}
```

To test it, open julia on the root folder and do

```
julia> A = rand(5,5); b = rand(5)
julia> include("src/solve_linear.jl")
julia> norm(linear_system(A,b) - A\b)
```

If the result is around $10^{-16}$, then everything went well.
Very rarely, the generated matrix could be ill-conditioned.
Run again, to verify if that's the case.

If everything went well, we'll write a test now.

For now, let's write a simple test running a lot of square linear systems.
For each system, to avoid using `A\b`, we'll create a vector `b` from a known
solution. Then we'll **assure** that $\Vert Ax-b\Vert < \epsilon$
and $\Vert x-e\Vert < \epsilon$.
To do that, we'll use `Base.Test`.
Note however, that the condition of the matrix influences the error, and there
are numerical errors involved. So we'll use the condition
$$\Vert x-e\Vert < 10^{-12} \text{cond}(A).$$
The code is

```
# file test/test1.jl
{% include_relative {{ page.name }}/test1.jl %}
```

Run with

```
$ julia test/test1.jl
```

Nothing should appear.

The first line is a kludge to read the correct file from wherever the run the
code. If you're not building a module, this is ok. But normally you'll want to
build a module. Ignore that for now.
The first for varies the dimension, and the second for runs the code a specific
number of times.
This totals a hundred square linear systems being run.
The `@test` macro verifies that the given expression is true.
If any solution is wrong, the code will be wrong. Also, if you use a smaller
tolerance, the numerical rounding may give a error here.

Ok, first thing you wanna do now is commit this code.

```
$ git init
$ git add src test
$ git commit -m 'First commit'
```

Then, go to GitHub, create an account, then a repository for this code (e.g.
linear_system.jl), then push the code.
**Use the name with .jl in the end for the repository.**

```
$ git add origin http://link/to/your/github/repository/
$ git push -u origin master
```

Enter your password and verify the code is online.

## Online testing

Now go to Travis and create an account. Go to your profile and
click on the `Sync account` button if necessary.
Find your repository and set the button to on.
Now, with the next commit, a test will start. Let's make it happen.

Create a file `.travis.yml` (yes, with a leading dot) with information for the
build. Here's a simple file:

```
# file .travis.yml
{% include_relative {{ page.name }}/travis1.yml %}
```

Include the file and push

```
$ git add .travis.yml
$ git commit -m 'Add .travis.yml'
$ git push
```

Now, go to your travis page, and after a while you'll see your repository with a
test running (or already finished, because it is short).
You should have a passing test. If not, verify your files again, then the error
on travis.
Notice that you can see the complete log of what is run.

Using an online automated testing is useful for many reasons:

- Everyone can see if the code is working;
- Pull requests generate a travis build, so you can see if it's working;
- You don't forget to test;
- You test on a clean environment;
- You can test with multiple versions of Julia (or other linguage).

## Coverage

Now, let's see the code coverage.
First, for coverage you'll need a package to see the coverage, and the service
to publish the coverage.

Use [Coverage.jl](https://github.com/JuliaCI/Coverage.jl) to see your coverage
(including locally).
Install with

```
julia> Pkg.add("Coverage")
```

Then run

```
$ julia --code-coverage=user --inline=no test/test1.jl
```

This will generate a file `src/solve_linear.jl.xxx.cov` with the information.
The option `--inline=no` gives more accurate results, but slow down the code.
You can see which function are not being run by reading it, but it's better to
see it online.

To see a summary, use

```
julia> using Coverage
julia> cov = process_folder()
julia> c, t = get_summary(cov)
julia> println("$(100c/t)% of lines covered")
```

But we want to see it online. So go to Coveralls.io and create an account.
Click on `Add repos` and find you repository.
Enable it, and change the `.travis.yml` file to

```
# file .travis.yml
{% include_relative {{ page.name }}/travis2.yml %}
```

After a success, we install Coverage and run the relevant code.
Check your repository on Coveralls to see the results.
Notice how the `error` line on our code never gets called.

## Improving to a module

If you want people to use your code, you should use a module in Julia.
This allows easy installation of your code, and not much more work.
Changing to a module is very simple, so I'll run through it.
The folders `src` and `test` are required. But we also need

- A file in src with the same name as the repository;
- The keyword `module` on that file;
- `export` the relevant functions;
- A file `test/runtests.jl` that run the tests;
- A `README.md` for people to know about your thing;
- A `LICENSE.md` for people to know what they can do with your file;
- Different `.travis.yml`.

I'm using the name `BlogAutomatedTesting.jl`, so a create the file

```
# file src/BlogAutomatedTesting.jl
{% include_relative {{ page.name }}/main.jl %}
```

I edit the file

```
# file src/solve_linear.jl
{% include_relative {{ page.name }}/solve_linear2.jl %}
```

Then file

```
# file test/runtests.jl
include("test1.jl")
```

and

```
# file test/test1.jl
{% include_relative {{ page.name }}/test12.jl %}
```

And create a `README.md`

```
# BlogAutomatedTesting.jl

This package was created from the tutorial on
[Abel Siqueira's blog]({{ site.url }}/{{local_prefix}}{{ post.url }})
```

The `LICENSE.md` file is up which license you'll choose.
See [this site](choosealicense.com) for options.
Copy the contents to the file.

Now change `.travis.yml` to treat your code like a package.

```
# file .travis.yml
{% include_relative {{ page.name }}/travis3.yml %}
```

Commit and verify your update on Travis and Coveralls

```
$ git add .
$ git commit -m 'Change to module'
$ git push
```

I hope this was helpful enough.
]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[VIII Simpósio de Análise Numérica e Otimização - Minicurso de Julia]]></title>
            <link>https://abelsiqueira.github.io/blog/2016-02-14-viii-simposio-de-analise-numerica</link>
            <guid>https://abelsiqueira.github.io/blog/2016-02-14-viii-simposio-de-analise-numerica</guid>
            <pubDate>Sun, 14 Feb 2016 00:00:00 GMT</pubDate>
            <content:encoded><![CDATA[
# VIII Simpósio de Análise Numérica e Otimização - Minicurso de Julia

Nos dias 22 a 24 de Fevereiro de 2016 acontecerá o [VIII Simpósio de Análise
Numérica e Otimização](http://www.mat.ufpr.br/verao/2016/m4_otimiza.html).
É uma ótima oportunidade para interagir com colegas e conhecer alguns assuntos
novos de pesquisa.

Em particular, nesse simpósio apresentarei um minicurso sobre Julia com foco em
Otimização. O minicurso é voltado para pessoas que conhecem um pouco de MatLab
ou alguma outra linguagem. Serão três dias, com uma introdução à linguagem,
alguns exemplos, o [CUTEst.jl](http://github.com/JuliaOptimizers/CUTEst.jl),
e um workflow para criar um pacote de otimização com transição para C e Fortran.

O material para o minicurso está disponível
[aqui](https://github.com/abelsiqueira/julia-workshop)
(neste momento ainda incompleto).
]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Installing CUTEst and CUTEst.jl]]></title>
            <link>https://abelsiqueira.github.io/blog/2015-10-01-installing-cutest-and-cutestjl</link>
            <guid>https://abelsiqueira.github.io/blog/2015-10-01-installing-cutest-and-cutestjl</guid>
            <pubDate>Thu, 01 Oct 2015 00:00:00 GMT</pubDate>
            <content:encoded><![CDATA[
# Installing CUTEst and CUTEst.jl

This post will tell you how to install CUTEst using a different tool that makes
it much easier. Also, I'll install CUTEst.jl, the CUTEst interface for Julia.

**Edit:** _Now, CUTEst.jl install CUTEst by itself. Check [this
post](https://abelsiqueira.github.io{{local_prefix}}nlpmodelsjl-cutestjl-and-other-nonlinear-optimization-packages-on-julia/).
Also, for Linux, I've created [this CUTEst
installer](https://github.com/abelsiqueira/linux-cutest), which should be
easier to use. February, 11, 2017_.

**Edit:** _Some corrections were made on February, 15, 2016_.

**Edit:** _Some corrections were made on November, 11, 2015_.

By now you probably know
[CUTEst](http://ccpforge.cse.rl.ac.uk/gf/project/cutest/wiki),
the repository for testing and comparing nonlinear programming algorithms.
It's widely used in the community for some time (considering CUTE and CUTEr,
the previous versions).
If not, this is a good change to test it, using
[Julia](http://www.julialang.org) to play around.
This is a not a post to convince you to use Julia, but I have to say that it is
much easier to use CUTEst on Julia than on MatLab.
So, if you are starting on it, I suggest you take a look.

We will use Homebrew to install CUTEst, for two reasons:

- It's much easier (when you learn it)
- Julia requires shared libraries, that the original installation did not
  provide.

Homebrew is a kind of package manager (such as apt-get, pip, etc.).
For linux, there are many things that we don't need from Homebrew, because you
normally already have a package manager. However, Homebrew is widely used by OSX
users, so it has a lot of packages.
The linux version is [Linuxbrew](https://github.com/Homebrew/linuxbrew).

The installation is quite simple:

- Install brew
- Install CUTEst
- Install CUTEst.jl

I just made these steps and record my terminal, so you can check
[Asciinema](https://asciinema.org/a/27127), or the embedded version on the
bottom of the page. Be warned, though, that I was "cold running" them, so some
parts are very slow.

To install brew, I recommend you check the page. For the impatient,

```
ruby -e "$(curl -fsSL https://raw.githubusercontent.com/Homebrew/linuxbrew/go/install)"
echo 'export PATH="$HOME/.linuxbrew/bin:$PATH"' >> $HOME/.bashrc
source $HOME/.bashrc
sudo apt-get install build-essential subversion
brew doctor
```

To install CUTEst, read the
[tap cutest](https://github.com/optimizers/homebrew-cutest).
Again, for the impatient

```
brew tap optimizers/cutest
brew install cutest
brew install mastsif
for f in archdefs mastsif sifdecode cutest; do \
  echo "source $(brew --prefix $f)/$f.bashrc" >> \
  $HOME/.bashrc; \
done
echo 'export LD_LIBRARY_PATH="$HOME/.linuxbrew/lib:$LD_LIBRARY_PATH"' >> $HOME/.bashrc
source $HOME/.bashrc
```

This should get CUTEst installed.
Notice the `LD_LIBRARY_PATH` variable, which points to where the CUTEst library
will be.

Test it with

```
brew test sifdecode
brew test cutest
```

That's it. You have CUTEst installed to use with Fortran or C.
A can't provide a simple example, because they aren't simple (enough).
I'll now go to Julia, and I recommend you try it.

To install Julia, go to their page, then downloads, then download the
static version of the stable release (or do what you want, I'm not your boss).
Then, in julia, to install
[CUTEst.jl](https://github.com/abelsiqueira/CUTEst.jl),
issue the commands

```
Pkg.clone("https://github.com/abelsiqueira/CUTEst.jl")
Pkg.checkout("CUTEst", "fix/issue4")
```

If nothing goes wrong, then you can play around.
For instance, to open problem HS32 and get the objective function value at point
(2,3), we do

```
using CUTEst
nlp = CUTEstModel("HS32")
f = obj(nlp, [2.0;3.0])
```

If you're familiar with CUTEst, you can use the classic functions `cfn` and
`ufn` too, in the default way (as called from C) or a more Julian way.
This would become too long to explain now, so I'll make a post in a few days (or
months).
If you need it, please contact me.

This concludes the new installation of CUTEst.

**Warning**: Due to current limitations we cannot open two problems at the same
time in CUTEst without the possibility of a segmentation fault.
So, if you need to run cutest for a list of problems, I suggest you use a bash
script to loop over each problem and call your Julia code passing the problem as
an input argument.

Ths embedded Asciinema video is below.

<script type="text/javascript" src="https://asciinema.org/a/27127.js"
id="asciicast-27127" async></script>
]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Workshop de Ferramentas Computacionais - Maio de 2015]]></title>
            <link>https://abelsiqueira.github.io/blog/2015-04-13-workshop-de-ferramentas-computacionais-maio-de-2015</link>
            <guid>https://abelsiqueira.github.io/blog/2015-04-13-workshop-de-ferramentas-computacionais-maio-de-2015</guid>
            <pubDate>Mon, 13 Apr 2015 00:00:00 GMT</pubDate>
            <content:encoded><![CDATA[
# Workshop de Ferramentas Computacionais - Maio de 2015

# Sobre

No dia 30 de Maio de 2015 acontecerá uma Oficina de Ferramentas Computacionais
para Pesquisadores.
Essa oficina será focada em três ferramentas básicas para desenvolvimento de
software:

- Bash
- Git
- Julia

O Bash é o terminal tradicinal dos sistemas GNU/Linux (a tela preta).
As possibilidades de uso são ilimitadas, mas vamos focar no básico,
tentando chegar até a criação de um script simples.

O Git é uma ferramenta para controlar versões de softwares e outros arquivos.
Você consegue guardar versões anteriores dos seus arquivos, assim como versões
alternativas, com uma estrutura escondida que melhora a organização e o design.
Ela é usada principalmente para controle de versões de software, permitindo que
cada programador saiba o que foi feito por quem e quando, mas também pode ser
utilizado para controle de versões de artigos, teses e apresentações.
O [modelo de tese do IMECC - UNICAMP](https://github.com/lpoo/modelo_tese_imecc)
usa o Git, e este site está [armazenado usando o
Git](https://github.com/abelsiqueira/abelsiqueira.github.io).
É uma das ferramentas principais para gerenciamento de versões, e [muitos
projetos importantes o usam](https://git.wiki.kernel.org/index.php/GitProjects).
O Git pode ser usado apenas em seu computador, mas o uso comum envolve algum
servidor remoto (principalmente para colaboração). A oficina tentará mostrar um
pouco de cada.

Julia é uma nova linguagem de programação que tem o objetivo de ser rápido como
C/Fortran, permitindo uma interfaca simples entre C e Fortran, e com sintaxe
parecida com a de Matlab e Python.
Como é uma linguagem livre, acreditamos ser uma escolha melhor que o Matlab para
software livre.
Também é uma boa linguagem inicial por ser prática, e tem muito espaço para
expansão por nova.

# Ministrantes

- [Eu]({{local_prefix}}/) - Trabalho com GNU/Linux desde minha graduação,

```
e trabalho com software livre. Meus trabalhos individuais são de código
aberto, e acredito que esse é o caminho mais correto para o desenvolvimento
acadêmico.
```

- [Raniere Gaia](http://rgaiacs.com/) - Estuda matemática aplicada na

```
Universidade Estadual de Campinas, e trabalha em alguns projetos de nível
internacional. Faz contribuições para Mozilla, e é um membro mantenedor do
Software Carpentry.
```

# Requerimentos

Para participar da oficina voce **precisa de um computador portátil**
com alguns softwares instalados.
**Idealmente**, um computador com algum tipo Unix (Ubuntu, Fedora, OSX) é melhor
para o desenvolvimento _na opinião do autor da página_.
[Veja
discussões](http://www.quora.com/Is-it-important-for-modern-programmers-to-know-use-Unix-Why).
Além disso, os instrutores não tem o conhecimento para dar suporte completo no
Windows (Aceitamos voluntários).
(Nota: Teremos alguns computadores com sistema operacional Windows disponíveis,
mas não teremos tempo para testar todos, ou instalar tudo que julgamos
essencial).

Softwares que você precisa

- Bash

```
- No GNU/Linux, o Bash já deve estar instalado.
Procure pelo `terminal` ou `console` no seu sistema. Quando ele abrir,
digite
```

```
bash --version
```

```
Uma mensagem deve aparecer indicando qual versão está instalada.
```

```
- No OSX, o terminal padrão também é o Bash. Você pode acessar pelo
`/Applications/Utilities/`. Dica: coloque o terminal no dock.
```

```
- No Windows existem algumas opções,
uma delas é o [Git Bash](http://msysgit.github.io/), que você já usa para o
Git também.
```

- Git

```
- No Windows, instale o [Git Bash](http://msysgit.github.io/), que falei
acima.
**Nota**: Uma das opções da instalações é sobre o fim de linha (line
ending). Escolha a opção de seguir o padrão Unix (ou algo parecido).
```

```
- No OSX, você pode instalar o Git para Mac baixando um instalador
[aqui](http://sourceforge.net/projects/git-osx-installer/files/).
```

```
- Num computador tipo Unix, siga as [instruções](http://git-scm.com/downloads).
Teste que o git foi instalado fazendo
```

```
git --version
```

```
no seu terminal.
```

- Julia: Siga as [instruções](http://julialang.org/downloads/).

```
Para testar, verifique que o Julia foi instalado abrindo o terminal de
Julia, e verificando que o texto introdutório contém `Version 0.x.x`.
```

- Um editor de texto, preferencialmente com suporte a UTF-8, código colorido

```
automaticamente, indentação automática e inclusão de espaços no lugar de
TAB. Existem algumas opções, e se você não tem preferência nenhuma, o
[atom.io](https://atom.io/) é um bom para começar. Se for usá-lo,
recomendamos buscar pelo pacote `language-julia`.
```

**Certifique-se de ter essas ferramentas instaladas ANTES do evento**. Entre em
contato em caso de dúvidas.

# Inscrições

As inscrições estão fechadas. Caso tenha interesse em participar, me mande um
e-mail para confirmar se ainda temos vagas.

# Cronograma

O evento acontecerá no dia 30 de Maio de 2015, seguindo o cronograma abaixo:
(_Nota: Mudamos o horário de Julia para melhor acomodar o cronograma_).

- **08:00** : Abertura
- **08:30** : Bash (com Raniere)
- **09:30** : _Coffee Break_
- **10:00** : Julia (com Abel)
- **11:30** : Almoço
- **13:30** : Git Local (com Abel)
- **15:00** : _Coffee Break_
- **15:30** : Git Remoto (com Raniere)

# Local

Utilizaremos o Laboratório de Matemática e Desenho (LAMADE) no bloco PC, sala
PC12.
]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Apresentação no VII Simpósio de Análise Numérica e Otimização - UFPR]]></title>
            <link>https://abelsiqueira.github.io/blog/2015-02-25-apresentacao-no-vii-simposio-de-analise-numerica-e-otimizacao-ufpr</link>
            <guid>https://abelsiqueira.github.io/blog/2015-02-25-apresentacao-no-vii-simposio-de-analise-numerica-e-otimizacao-ufpr</guid>
            <pubDate>Wed, 25 Feb 2015 00:00:00 GMT</pubDate>
            <content:encoded><![CDATA[
# Apresentação no VII Simpósio de Análise Numérica e Otimização - UFPR

No dia 24 de Fevereiro de 2015 aconteceu o (primeiro dia do) VII Simpósio de
Análise Numérica e Otimização.
Participei deste congresso fazendo uma apresentação sobre
[Ferramentas Computacionais para
Pesquisadores](/blog/2015-02-25-vii-simposio.pdf)
(ver [código](http://github.com/abelsiqueira/pres-ferramentas-computacionais)).

Nesta apresentação, introduzo algumas ferramentas que considero bastante
importantes para pesquisadores, principalmente da área de matemática
computacional.
Um resumo do que apresento é

- Aprenda outras linguagens. Recomendo, por exemplo, conhecer Python ou Ruby,

```
Shell e Makefile. Com essas ferramentas já é possível automatizar testes e
fazer scripts com uso variado. Makefile, por exemplo, é uma das maneiras
mais usadas de se instalar programas (no Unix).
```

- Conheça a linguagem [Julia](http://www.julialang.org).

```
É uma linguagem com foco na matemática computacional, considerando ainda que
você irá querer utilizar código em C e Fortran, e com sintaxe parecido com a
de MatLab/Octave. É uma linguagem nova, mas tem potencial para ser o próximo
concorrente do MatLab/Octave, e é livre.
- Dentro do Julia, conheça o [JuliaOpt](http://www.juliaopt.org), que é
um grupo que está desenvolvendo ferramentas de otimização em Julia.
Desde interfaces para softwares conhecidos, até linguagens de modelagem,
passando por implementações de métodos de otimização não-linear, tanto
puramente em Julia, quanto utilizando códigos de baixo nível.
- Também anunciei que estamos trabalhando no
[CUTEst.jl](http://github.com/optimizers/CUTEst.jl), uma interface para o
[CUTEst](http://ccpforge.cse.rl.ac.uk/gf/project/cutest/wiki).
```

- Escolha um editor e um ambiente de desenvolvimento.

```
Sugiro ver algumas opções (Vim, Emacs, Atom, Sublime Text, Eclipse), testar,
e ver qual combina mais com você. Em adição, conheça o Sharelatex e/ou o
Writelatex.
```

- Conheça o [perprof-py](http://github.com/abelsiqueira/perprof-py),

```
que é uma ferramenta para gerar perfis de desempenho, com gráficos de alta
qualidade.
```

- Conheça o [git](http://git-scm.com), que é uma ferramenta para controle de

```
versão, que você pode usar sozinho; ou em grupo; fazer ramificações;
verificar versões anteriores; misturar versões; trabalhar online; dentro
outras.
- Conheça o [GitHub](http://github.com), que é um site onde você pode
colocar o código que foi feito com git.
- Conheça o [Travis CI](http://travis-ci.org), que é um serviço que baixa
seu código do GitHub e roda testes (definidos por você) nele, sempre que
você subir o seu código.
- Conheça o [Coveralls.io](http://coveralls.io), que é um serviço que
verifica seus testes e diz que parte do seu código foi verificado, e qual
não foi.
```

- TikZ e PgfPlots do Latex: Comentei um pouco sobre gráficos usando esses

```
pacotes, e mostrei alguns exemplos.
```

Também falei sobre o [Software Carpentry](http://www.software-carpentry.org),
que é uma organização sem fins lucrativos que realiza workshops e promove
conhecimento mundialmente. Comentei também sobre o trabalho do
[Raniere Silva](http://rgaiacs.com) no Software Carpentry, e do
[trabalho que ele está
propondo](http://catarse.me/pt/programacaocientifica) para os próximos meses.

Algumas boas perguntas foram feitas, e gostaria de parafraseá-las e
atualizar minha resposta para algumas delas.

- **Por que sair do MatLab para o Julia?**

```
Recomendo sair do MatLab porque ele é um software proprietário, pra começar.
Mas desconsiderando isso, e também considerando o Octave, dou a seguinte
resposta: O Julia está sendo desenvolvido com o intuito de substituir o
MatLab, sabendo que o matemático computacional costuma fazer código que
precisa de velocidade em C ou Fortran.
A interface para C e Fortran em Julia é consideravelmente fácil,
e isso facilita o processo de criar um código que você vê que funciona, e
posteriormente otimizá-lo.
Gostaria de acrescentar a ressalva que Julia é uma linguagem nova, e
obviamente não tem tudo que gostaríamos implementado. Seu código pode
quebrar. Mas vale a pena conhecer para saber se vale investir.
```

- **Por que sair do Python para o Julia?**

```
Não sei se você deve. O Python básico não é suficiente para um matemático
computacional, mas eu sei que existem vários pacotes que conseguem deixar o
Python muito eficiente para Análise Numérica e Otimização. Também é possível
fazer uma transição C com Python, mas não conheço, logo não posso julgar.
Atualmente, provavelmente, o Python parece ser mais eficiente.
E já que estamos aqui, se você conseguir tirar alguém do MatLab para o
Python, já é uma vitória.
```

- \*\*Por que usar o TikZ/PgfPlots no lugar desta outra ferramenta de

```
gráficos?**
Se for o MatLab, vide meu _rant_ anterior sobre MatLab ser proprietário.
Para outras ferramentas, não sei. Muitas ferramentas fazem um trabalho, no
mínimo, tão bom quanto o TikZ/PgfPlots, e.g., o MatplotLib (que
usamos no perprof, junto com o PgfPlots).
Uma coisa que eu gosto é misturar com o Beamer, gerando figuras iterativas
(não interativas).
Então, alguns comandos podem ser misturados para fazer uma sequência de
figuras que o Beamer vai descobrindo (por exemplo o caminho de um
algoritmo).
```

- \*\*Se fosse para você escolher apenas uma dessas ferramentas para

```
recomendar, qual seria?**
Sem dúvida o git, que é útil para qualquer área onde você escreve código, ou
até mesmo um artigo em .tex (ou outros formatos binários de texto).
Você nunca sabe quando vai precisar voltar numa versão anterior do código.
Por exemplo, você faz atualizações no seu código, e alguém diz que estava
usando o seu código antes, mas depois que essa pessoa atualizou, o código
parou de funcionar.
Bem, como fazer pra saber o que quebrou o código? Se você está fazendo o
controle corretamente, você terá vários _commits_ indicando o trabalho
feito. Você pode navegar nesses commits e descubrir a última versão que
funcionava. Assim você reduz consideravelmente a quantidade de código que
pode ter estragado o seu pacote.
Além disso, você pode trabalhar com versões paralelas, colaborativamente, e
ainda aproveitar de serviços fantásticos para quem usa git (GitHub, Travis,
Coveralls).
```

É importante ficar claro que eu não estou apresentado as melhores ferramentas
para seus respectivos objetivos, apenas aquelas que eu conheço e que podem
resolver o problema. Talvez alguma ferramenta seja melhor do que a que eu
apresentei, mas o ponto é você conhecer alguma ferramenta, e às vezes conhecer
alguma nova.
]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[CUTEst.jl]]></title>
            <link>https://abelsiqueira.github.io/blog/2015-02-06-cutestjl</link>
            <guid>https://abelsiqueira.github.io/blog/2015-02-06-cutestjl</guid>
            <pubDate>Fri, 06 Feb 2015 00:00:00 GMT</pubDate>
            <content:encoded><![CDATA[
# CUTEst.jl

About an year ago,
[Raniere](http://rgaiacs.com/)
started working on a interface for
[CUTEst](http://ccpforge.cse.rl.ac.uk/gf/project/cutest/wiki/).
He decided to create [ugly](https://github.com/lpoo/ugly),
a repository for CUTEst, but following the Unix procedure for
building packages (`./configure, make, make install`).
Also with ugly, he wanted to enable building a shared library
to be used with Julia.
This approach worked, but maintaining it is troublesome,
since it would require updating and testing of ugly for every
update of CUTEst.

What I decided to do was find a way to create a shared library
from a working CUTEst installation.
This focuses on another principle: passing the blame, er,
I mean, modularity.
My package would simply take a working CUTEst and make a
working shared library from it.
It also served of downloading and installing a new CUTEst
installation, since this would be required for testing.
The work can be found at
[cutest-julia-installer](http://github.com/abelsiqueira/cutest-julia-installer).

The second thing Raniere worked was the interface itself,
which is a module/package for Julia that enables
building a problem from its name,
retrieving its parameters,
and using its mathematical functions
(objective function, gradient, Hessian, constraints,
Jacobian, and so on).
I continued this work, changing the way the problem is built
(to use my shared library),
and translating the core functions to Julia.
With these additions, the usual functions on CUTEst can be
called with little change in Julia.

The next step is to facilitate the use of CUTEst functions
by creating higher-level interfaces.
So, instead of manually verifying if problem is
constrained or not, and then calling
`cfn(st, n, m, x, f, c)` or `ufn(st, n, x, f)`,
to get the objective function value,
one might simply call
`f = obj_fun(prob, x)`.
This should probably be slower,
if the user, for instance, ends up calling two functions
instead of one, but if it increases development time,
then it has server its purpose.
]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[A Study in Julia]]></title>
            <link>https://abelsiqueira.github.io/blog/2015-01-22-a-study-in-julia</link>
            <guid>https://abelsiqueira.github.io/blog/2015-01-22-a-study-in-julia</guid>
            <pubDate>Thu, 22 Jan 2015 00:00:00 GMT</pubDate>
            <content:encoded><![CDATA[
# A Study in Julia

Today I begin a study in [Julia](http://julialang.org/).
This fantastic tool has syntax similar to that of Octave/Matlab,
but is much faster. Furthermore, the interface with functions
made in C and Fortran is much easier to accomplish, and since
most things in computational mathematics are on these languages,
this feature is wonderful.

My intented work is

- make a simple julia and C interface, with auto-compiling

```
and test on GitHub and Travis CI;
```

- develop a nonlinear optimization tool completely in Julia,

```
then improve the slow bits by using C and/or Fortran;
```

- implement/improve the CUTEst interface

```
[[1]](https://github.com/abelsiqueira/ugly),
[[2]](https://github.com/abelsiqueira/CUTEst.jl),
possibly creating a SIF converter.
```

- if things work out, submit to [JuliaOpt](http://www.juliaopt.org/).

My work starts with the [Julia-C
Samples](https://github.com/abelsiqueira/julia-c-sample.git).
If you need me, I might be on `#julia` on IRC/freenode.
]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[The End of Fortran]]></title>
            <link>https://abelsiqueira.github.io/blog/2014-11-22-the-end-of-fortran</link>
            <guid>https://abelsiqueira.github.io/blog/2014-11-22-the-end-of-fortran</guid>
            <pubDate>Sat, 22 Nov 2014 00:00:00 GMT</pubDate>
            <content:encoded><![CDATA[
# The End of Fortran

One of my main interest is the end of Fortran **as the only programming language
used by mathematicias**. If you're not in the area of mathematical programming,
you probably find it strange for me to be talking about Fortran, so I explain.
Fortran is considered the fastest programming language in mathematics, and most
things made are in this language. Other often used languages such as
MatLab/Octave and Mathematica are high-level, which means they are slower and
are not considered for serious applications.

In reality, Fortran is one of the fastest programming language **for linear
algebra with dense matrices**. Since most things in this area are matrices, and
most vectors are dense, this can't be overlooked. In addition, fortran has a
relatively easy way to go, bypassing the painstankingly formatting and GOTO,
which were left behind by the newer versions.

The problem is that, since learning Fortran is almost imperative in the area,
there is little else used to code in the area. Hence, easy tasks without need of
fast execution become hard programming jobs. And furthermore, tasks that need
fast execution, but can't (shouldn't) be implement as matrices, become slow in
Fortran, and sometimes this is overlooked.

My proposition is **use the best tool for the job**, which is just common sense,
but applied to our area. Try [Python](http://www.python.org). Try
[Ruby](http://www.ruby-lang.org). Try [Julia](http://www.julialang.org). Try
to implementing a graph using a list of pointers. Try to implement a argument
parser. Try to implement reading, storing, and calculating the transpose of a
sparse matrix. Try to implement a tool to read all citations on your .tex, read
from a bibtex file such references and print then all in a nice format.

Now, rather than stay in that language, go for another. Try other languages.
When the job comes, you can choose the best tool for it. And you can combine.
Some languages, such as Julia, Fortran and C, are very easy to combine with
others. Implement your algorithm in Julia, then improve it later with Fortran.
And if you are really in need of speed, why not implement the hardest part in
machine language (ASSEMBLY)?
Implementing the fastest possible algorithm is very good for a specific
application, but in our line of work, we must take into account the method, not
only the implementation.
]]></content:encoded>
        </item>
    </channel>
</rss>